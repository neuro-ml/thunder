{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>You saw the lightning. Now it's time to hear the thunder \ud83c\udf29\ufe0f</p>"},{"location":"#thunder","title":"Thunder","text":"<p>\ud83c\udf29\ufe0f The Deep Learning framework based on Lightning</p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install thunder\n</code></pre> <p>Currently thunder is not published on pypi. Install it via git clone.  </p>"},{"location":"#start-experimenting","title":"Start experimenting","text":"<p>Many frameworks provide you with interfaces for your models and training pipelines, but we  have yet to see any tools for creating whole experiment.</p> <p>With :thunder: it's as simple as 1, 2, 3:</p> <ol> <li> <p>Create a config (e.g. <code>base.config</code>):     <pre><code>from myproject import MyDataset, MyModule\nfrom lightning import Trainer\nfrom torch.utils.data import DataLoader\n\n# these 3 fields are required\ntrain_data = DataLoader(MyDataset())\nmodule = MyModule()\ntrainer = Trainer()\n</code></pre></p> </li> <li> <p>Build the experiment:    <pre><code>thunder build base.config /path/to/some/folder\n</code></pre></p> </li> <li>Run it     <pre><code>thunder run /path/to/some/folder\n</code></pre></li> </ol> <p>Also, 2 and 3 can be combined into a single command: <pre><code>thunder build-run base.config /path/to/some/folder\n</code></pre></p> <pre><code>$ thunder build base.config /path/to/some/folder\n</code></pre>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#thundermodule","title":"ThunderModule","text":""},{"location":"#metricmonitor","title":"MetricMonitor","text":""},{"location":"#experiment-configs","title":"Experiment configs","text":""},{"location":"#cli-integrations-with-wandb","title":"CLI &amp; Integrations with WandB","text":""},{"location":"utils/","title":"Utils","text":"<p>Some useful utility functions. </p>"},{"location":"utils/#thunder.torch.utils","title":"<code>thunder.torch.utils</code>","text":""},{"location":"utils/#thunder.torch.utils.get_device","title":"<code>get_device(x)</code>","text":"<p>Infer device of torch.Tensor or nn.Module instance. Parameters</p> <p>x: Union[torch.Tensor, nn.Module] Returns</p> <p>device: torch.device</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def get_device(x: Union[torch.Tensor, nn.Module]) -&gt; torch.device:\n    \"\"\"\n    Infer device of torch.Tensor or nn.Module instance.\n    Parameters\n    ----------\n    x: Union[torch.Tensor, nn.Module]\n    Returns\n    -------\n    device: torch.device\n    \"\"\"\n    if isinstance(x, (torch.Tensor, LightningModule)):\n        return x.device\n    elif isinstance(x, nn.Module):\n        try:\n            return next(x.parameters()).device\n        except StopIteration as e:\n            raise RuntimeError(\"Can't infer the device, because the module has no parameters\") from e\n\n    raise TypeError(f\"Can't infer the device of {type(x)}\")\n</code></pre>"},{"location":"utils/#thunder.torch.utils.last_checkpoint","title":"<code>last_checkpoint(root)</code>","text":"<p>Load most fresh last.ckpt file based on time. Parameters</p> <p>root: Union[Path, str]     Path to folder, where last.ckpt supposed to be. Returns</p> <p>checkpoint_path: Union[Path, str]     If last.ckpt exists - returns Path to it. Otherwise, returns 'last'.</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def last_checkpoint(root: Union[Path, str]) -&gt; Union[Path, str]:\n    \"\"\"\n    Load most fresh last.ckpt file based on time.\n    Parameters\n    ----------\n    root: Union[Path, str]\n        Path to folder, where last.ckpt supposed to be.\n    Returns\n    -------\n    checkpoint_path: Union[Path, str]\n        If last.ckpt exists - returns Path to it. Otherwise, returns 'last'.\n    \"\"\"\n    checkpoints = [p for p in Path(root).glob(\"**/*.ckpt\") if p.name != \"last.ckpt\"]\n    if not checkpoints:\n        return \"last\"\n    return max(checkpoints, key=lambda t: os.stat(t).st_mtime)\n</code></pre>"},{"location":"utils/#thunder.torch.utils.maybe_from_np","title":"<code>maybe_from_np(*x, device='cpu')</code>","text":"<p>Recursively converts numpy arrays to torch.Tensor. Parameters</p> <p>*x: Any device: Union[torch.device, str]     Device to move to, default is CPU. Returns</p> <p>Collection of tensors. Examples</p> <p>x, y  # np.ndarray z = maybe_from_np(x) # convert to torch.Tensor x, y = maybe_from_np(x, y) # x and y are now tensors x, y, z = maybe_from_np(x, y, z) # maybe_from_np converts np arrays and tensors and does not affect other types dict_of_tensors = to_np(dict_of_np) # maybe_from_np converts any collection</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def maybe_from_np(*x: Any, device: Union[torch.device, str] = \"cpu\") -&gt; Any:\n    \"\"\"\n    Recursively converts numpy arrays to torch.Tensor.\n    Parameters\n    ----------\n    *x: Any\n    device: Union[torch.device, str]\n        Device to move to, default is CPU.\n    Returns\n    -------\n    Collection of tensors.\n    Examples\n    -------\n    &gt;&gt;&gt; x, y  # np.ndarray\n    &gt;&gt;&gt; z = maybe_from_np(x) # convert to torch.Tensor\n    &gt;&gt;&gt; x, y = maybe_from_np(x, y) # x and y are now tensors\n    &gt;&gt;&gt; x, y, z = maybe_from_np(x, y, z) # maybe_from_np converts np arrays and tensors and does not affect other types\n    &gt;&gt;&gt; dict_of_tensors = to_np(dict_of_np) # maybe_from_np converts any collection\n    \"\"\"\n    def to_tensor(x):\n        if isinstance(x, torch.Tensor):\n            return x.to(device)\n        return torch.from_numpy(x).to(device)\n    return squeeze_first(apply_to_collection(x, (np.ndarray, np.generic, torch.Tensor), to_tensor))\n</code></pre>"},{"location":"utils/#thunder.torch.utils.tensor2np","title":"<code>tensor2np(x)</code>","text":"<p>Detaches, moves torch.Tensor to CPU and converts into numpy array. Parameters</p> <p>x: torch.Tensor Returns</p> <p>np.ndarray</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def tensor2np(x: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"\n    Detaches, moves torch.Tensor to CPU and converts into numpy array.\n    Parameters\n    ----------\n    x: torch.Tensor\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\n    return x.detach().cpu().numpy()\n</code></pre>"},{"location":"utils/#thunder.torch.utils.to_np","title":"<code>to_np(*x)</code>","text":"<p>Converts collection of tensors into numpy arrays. Parameters</p> <p>*x: Any Returns</p> <p>Collection of numpy arrays Examples</p> <p>x, y  # torch.Tensor z = to_np(x) # convert to numpy array x, y = to_np(x, y) # x and y are now numpy arrays x, y, z = to_np(x, y, z) # to_np converts only tensors and does not affect other types dict_of_np = to_np(dict_of_tensors) # to_np converts any collection</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def to_np(*x: Any) -&gt; Any:\n    \"\"\"\n    Converts collection of tensors into numpy arrays.\n    Parameters\n    ----------\n    *x: Any\n    Returns\n    ----------\n    Collection of numpy arrays\n    Examples\n    --------\n    &gt;&gt;&gt; x, y  # torch.Tensor\n    &gt;&gt;&gt; z = to_np(x) # convert to numpy array\n    &gt;&gt;&gt; x, y = to_np(x, y) # x and y are now numpy arrays\n    &gt;&gt;&gt; x, y, z = to_np(x, y, z) # to_np converts only tensors and does not affect other types\n    &gt;&gt;&gt; dict_of_np = to_np(dict_of_tensors) # to_np converts any collection\n    \"\"\"\n    return squeeze_first(apply_to_collection(x, torch.Tensor, tensor2np))\n</code></pre>"},{"location":"callbacks/","title":"Overview","text":"<p>Lightning Callbacks allow you to modify your training pipelines. For extra information see this.</p>"},{"location":"callbacks/#thunder-callbacks","title":"Thunder Callbacks","text":"Name Description MetricMonitor Computes metrics and logs them TimeProfiler Logs the time of each LightningModule's step FailOnInterrupt Forces lightning Trainer to fail on KeyboardInterrupt"},{"location":"callbacks/fail_on_interrupt/","title":"FailOnInterrupt","text":"<p>Forces lightning Trainer to fail on KeyboardInterrupt by raising RuntimeError.</p>"},{"location":"callbacks/fail_on_interrupt/#usage","title":"Usage","text":"<pre><code>from lightning import Trainer\nfrom thunder.callbacks import FailOnInterrupt\n\ntrainer = Trainer(..., callbacks=[FailOnInterrupt()])\n</code></pre>"},{"location":"callbacks/fail_on_interrupt/#reference","title":"Reference","text":""},{"location":"callbacks/fail_on_interrupt/#thunder.callbacks.fail_on_interrupt.FailOnInterrupt","title":"<code>thunder.callbacks.fail_on_interrupt.FailOnInterrupt</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Forces RuntimeError in order for trainer to stop if KeyboardInterrupt was raised</p> Source code in <code>thunder/callbacks/fail_on_interrupt.py</code> <pre><code>class FailOnInterrupt(Callback):\n    \"\"\"Forces RuntimeError in order for trainer to stop if KeyboardInterrupt was raised\"\"\"\n\n    def on_exception(self, trainer: Trainer, pl_module: LightningModule, exception: BaseException) -&gt; None:\n        if isinstance(exception, KeyboardInterrupt):\n            raise RuntimeError(\"Finished run on KeyboardInterrupt\") from exception\n</code></pre>"},{"location":"callbacks/fail_on_interrupt/#thunder.callbacks.fail_on_interrupt.FailOnInterrupt.on_exception","title":"<code>on_exception(trainer, pl_module, exception)</code>","text":"Source code in <code>thunder/callbacks/fail_on_interrupt.py</code> <pre><code>def on_exception(self, trainer: Trainer, pl_module: LightningModule, exception: BaseException) -&gt; None:\n    if isinstance(exception, KeyboardInterrupt):\n        raise RuntimeError(\"Finished run on KeyboardInterrupt\") from exception\n</code></pre>"},{"location":"callbacks/metric_monitor/","title":"MetricMonitor","text":"<p>This callback takes on computation and aggregation of the specified metrics.  </p>"},{"location":"callbacks/metric_monitor/#usage","title":"Usage","text":""},{"location":"callbacks/metric_monitor/#loss","title":"Loss","text":"<p>Despite the word <code>Metric</code> in the name, this callback also takes on logging of train loss(es). It casts them by the following rules: - If <code>loss</code> is of type <code>torch.Tensor</code> - <code>{\"loss\": loss}</code> is logged. - If <code>loss</code> is a list or tuple, then it is logged as <code>{\"i\": loss_i}</code>. - If <code>loss</code> is a dict, then it is logged as is.  </p> <p> </p> <p>All Tensors are cast to numpy arrays.</p> <p>At the end of epoch they are averaged and sent to logger.</p>"},{"location":"callbacks/metric_monitor/#metric-computation","title":"Metric Computation","text":"<p>Metrics are assumed to be received as tuple <code>(X, Y)</code>, where X - batch of predictions, Y - batch of targets.  Further process of computation depends on whether <code>Group</code> or <code>Single</code> metrics are used. Also, there is no difference for MetricMonitor between  <code>(X, Y)</code> and <code>((X,), (Y,))</code>. If your model has multiple outputs or requires multiple targets (e.g. neural network with 2 heads.), the output is expected to be <code>((X1, X2), (Y1, Y2))</code> (the most common way to represent such data in PyTorch), where X1 is batch of model's first output. In this case outputs will be recombined, so the first object of the output will be <code>((x1, x2), (y1, y2))</code>, where x1 is the first element of batch <code>X1</code>.  </p> <p> </p> <p>Inside the callback outputs are swapped, so if LightningModule returns (X, Y) then metrics will receive (Y, X).</p>"},{"location":"callbacks/metric_monitor/#group-metrics","title":"Group metrics","text":"<p>Group metrics are computed on the entire dataset. For example, you want to compute classification accuracy on MNIST.</p> <pre><code>from thunder.callbacks import MetricMonitor\nfrom sklearn.metrics import accuracy_score\n\ntrainer = Trainer(callbacks=[MetricMonitor(group_metrics={\"accuracy\": accuracy_score})])\n</code></pre> <p>If you use any loggers (e.g. <code>Tensorboard</code> or <code>WandB</code>), <code>accuracy</code> will appear in them as follows: <code>val/accuracy</code> - validation metrics. <code>test/accuracy</code> - test metrics.</p> <p>You can also use preprocessing functions as keys of the dictionary. It is  covered in Preprocessing part in Single Metrics paragraph. Here is simple example <pre><code>from sklearn.metrics import accuracy_score, recall_score\n# y is binary label\n# x is e.g. a binary tensor and we want to know if there is any true value in it.\nthreshold = lambda y, x: (y, x.any())\n\ngroup_metrics = {threshold: [accuracy_score, recall_score]}\n</code></pre> Despite group metrics being calculated on collections of entries, preprocessing is applied individually.</p>"},{"location":"callbacks/metric_monitor/#single-metrics","title":"Single metrics","text":"<p>Single metrics are computed on each object separately and only then aggregated. It is a common use case for tasks like segmentation or object detection.</p>"},{"location":"callbacks/metric_monitor/#simple-use-case","title":"Simple use case","text":"<p><pre><code>from thunder.callbacks import MetricMonitor\nfrom sklearn.metrics import accuracy_score\n\ntrainer = Trainer(callbacks=[MetricMonitor(single_metrics={\"accuracy\": accuracy_score})])\n</code></pre> MetricMonitor will log mean values by default. But you can add custom aggregations as well.</p>"},{"location":"callbacks/metric_monitor/#custom-aggregations","title":"Custom aggregations","text":"<p>Let see what can be done if we want to log <code>std</code> of metrics as well as mean values.</p> <p><pre><code>import numpy as np\nfrom thunder.callbacks import MetricMonitor\nfrom sklearn.metrics import accuracy_score\n\naggregate_fn = np.std\n\nmetric_monitor = MetricMonitor(single_metrics={\"accuracy\": accuracy_score},\n                              aggregate_fn=aggregate_fn)\n\ntrainer = Trainer(callbacks=[metric_monitor])\n</code></pre> The mean values appear in loggers with no additional keys.  MetricCallback will try to infer the name of an aggregating function and use it as an additional key.</p> <p><code>val/accuracy</code> - validation mean accuracy. <code>val/std/accuracy</code> - validation accuracy std. <code>test/accuracy</code> - test mean accuracy. <code>test/std/accuracy</code> - test accuracy std.</p> <p><code>aggregate_fn</code> can also be specified as follows:</p> <p><pre><code>import numpy as np\n\naggregate_fn = [np.std, np.median]\naggregate_fn = [np.std, \"median\", \"max\", \"min\"]\naggregate_fn = {\"zero\": lambda x: x[0]}\n</code></pre> MetricMonitor can accept <code>str</code> or <code>List[str]</code> as <code>aggregate_fn</code>,  in this format it supports the following metrics:</p> Name Function \"median\" <code>np.median</code> \"min\" <code>np.min</code> \"max\" <code>np.max</code> \"std\" <code>np.std</code>"},{"location":"callbacks/metric_monitor/#preprocessing","title":"Preprocessing","text":"<p>Sometimes metrics require some preprocessing. In this case, keys of <code>single_metrics</code> dict must be callable objects. <pre><code>from sklearn.metrics import accuracy_score, recall_score\n\nthreshold = lambda y, x: (y &gt; 0.5, x)\n\nsingle_metrics = {threshold: [accuracy_score, recall_score]} \n# or\nsingle_metrics = {threshold: {\"acc\": accuracy_score, \"rec\": recall_score}}\n# or\nsingle_metrics = {threshold: recall_score}\n...\n</code></pre> In the example above, <code>accuracy_score</code> and <code>recall_score</code> are computed on each case separately  (e.g. like in semantic segmentation task).  Preprocessing functions are applied on each entry separately in both <code>single_megtrics</code> and <code>group_metrics</code>.</p>"},{"location":"callbacks/metric_monitor/#individual-metrics","title":"Individual Metrics","text":"<p>While computing <code>single_metrics</code>, one may appear in need of knowledge of metrics on each case. For this particular problem, the callback provides its users with <code>log_individual_metrics</code> flag. Being set to <code>True</code> it forces the callback to store table of metrics in the following format:</p> Name metric1 metric2 batch_idx0_0 some_value some_value batch_idx0_1 some_value some_value ... ... ... batch_idxn_m some_value some_value <p>For each set (e.g. <code>val</code>, <code>test</code>) and each <code>dataloader_idx</code>, MetricMonitor stores separate table. By default aforementioned tables are saved to <code>default_root_dir</code> of lightning's Trainer, in the format of <code>set_name/dataloader_idx.csv</code> (e.g. <code>val/dataloader_0.csv</code>). If loggers you use have method <code>log_table</code> (e.g. <code>WandbLogger</code>),  then this method will receive key and each table in the format of <code>pd.DataFrame</code>. Code from <code>metric_monitor.py</code>: <pre><code>logger.log_table(f\"{key}/dataloader_{dataloader_idx}\", dataframe=dataframe)\n</code></pre> where key is the current state of trainer (<code>val</code> or <code>test</code>).  </p> <p>Since lightning allows to use <code>batch_idx</code>, these indexes are used for metrics dataframes. But there can be more than one object in batch. To overcome this issue we iterate over batch and mark each object with the next index:  <pre><code>for i, object in enumerate(batch):\n    object_idx = f\"{batch_idx}_{i}\"\n</code></pre> If all batches consist of single object, then <code>\"_{i}\"</code> is removed.</p>"},{"location":"callbacks/metric_monitor/#reference","title":"Reference","text":""},{"location":"callbacks/metric_monitor/#thunder.callbacks.metric_monitor.MetricMonitor","title":"<code>thunder.callbacks.metric_monitor.MetricMonitor</code>","text":"<p>             Bases: <code>Callback</code></p>"},{"location":"callbacks/time_profiler/","title":"TimeProfiler","text":"<p>Lightning Callback which allows you to measure the time each step takes and log it during the training process.</p>"},{"location":"callbacks/time_profiler/#logged-values","title":"Logged values","text":"<p>TimeProfiler logs the following steps:</p> Name Logged by default Description train batch Time taken by forward, optimizer step, and backward during train step. validation batch Time taken by forward during validation step. train epoch Time taken by train epoch without validation. validation epoch Time taken by validation epoch. avg train downtime* Average downtime in training step. avg val downtime Average downtime in validation step. backward Time taken by backprop. optimizer step Time taken by optimizer. total train downtime Total downtime in training epoch. total val downtime Total downtime in validation epoch. <p>*Downtime - the process during which model does not work (e.g. data loader is working now)  </p>"},{"location":"callbacks/time_profiler/#usage","title":"Usage","text":"<pre><code>from thunder.callbacks import TimeProfiler\nfrom lightning import Trainer\n\n# logs default keys and in addition backward and optimizer step\ntrainer = Trainer(callbacks=[TimeProfiler(\"backward\", \"optimizer step\")])\n</code></pre>"},{"location":"callbacks/time_profiler/#reference","title":"Reference","text":""},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler","title":"<code>thunder.callbacks.time_profiler.TimeProfiler</code>","text":"<p>             Bases: <code>Callback</code></p>"},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler--parameters","title":"Parameters","text":"<p>keys : Union[str, bool]     Optional keys for logging. If set to <code>True</code> it will log all keys.</p>"},{"location":"cli/","title":"Command Line Interface","text":""},{"location":"cli/#requirements-lazycon","title":"Requirements: lazycon","text":"<p>Thunder provides its users with CLI to bring convenience and comfort into experiment building and execution routine.</p> <p>For any help you can use <pre><code>thunder --help\n</code></pre></p>"},{"location":"cli/#building-an-experiment","title":"Building an experiment","text":"<p>In order to build an experiment, you can execute the follwing command: <pre><code>thunder build /path/to/config /path/to/experiment\n</code></pre> It will create a folder with built configs in it. If <code>/path/to/experiment</code> already exists, thunder raises error.  In order to overwrite existing directory use <code>--overwrite</code> / <code>-o</code> flags.</p>"},{"location":"cli/#overriding-config-entries","title":"Overriding config entries","text":"<p>While conducting experiments one can find themselves in constant need of changing significant number of parameters. But it is not convenient to always do it via IDE or any other code editor.  Thunder gives an ability to override the values while building an experiment.</p> <p>If in your config you have <pre><code>batch_size = 1\nlr = 0.01\n</code></pre> You can override it using <code>-u</code> flag: <pre><code>thunder build /path/to/config /path/to/experiment -u batch_size=2 -u lr=0.001 \n</code></pre> <code>batch_size</code> and <code>lr</code> will be assigned 2 and 0.001 respectively.</p>"},{"location":"cli/#running-an-experiment","title":"Running an experiment","text":"<p>You can run built experiment by executing the next command: <pre><code>thunder run /path/to/experiment\n</code></pre> Under the hood thunder extracts necessary entries (e.g. model and trainer) from your built config and executes <code>trainer.run(model, train_data, ...)</code>.</p>"},{"location":"cli/#backend","title":"Backend","text":"<p>As default options Thunder supports several backends: - cli (default) - slurm</p> <p>You can switch between them by specifying <code>--backend</code> flag.  <pre><code>thunder run /path/to/experiment/ --backend slurm -c 4 -r 100G \n</code></pre> The command shown above will run SLURM job with 4 CPUs and 100G of RAM.</p>"},{"location":"cli/#predefined-run-configs","title":"Predefined run configs","text":"<p>You can predefine run configs to avoid reentering the same flags. Create <code>~/.config/thunder/backends.yml</code> (you can run <code>thunder show</code> in your terminal,  required path will be at the title of the table) in you home directory.  Now you can specify config name and its parameters: <pre><code>run_config_name:\n  backend: slurm\n  config:\n    ram: 100G\n    cpu: 4\n    gpu: 1\n    partition: partition_name\n</code></pre> In order to run an experiment with predefined parameters,  use <code>--backend</code> flag as in previous section:</p> <p><pre><code>thunder run /path/to/experiment/ --backend run_config_name\n</code></pre> You can overwrite parameters if you want to (e.g. 8 CPUs instead of 4): <pre><code>thunder run /path/to/experiment/ --backend run_config_name -c 8\n</code></pre></p>"},{"location":"cli/#add-set-list-remove","title":"Add, Set, List, Remove","text":"<p><code>thunder</code> CLI provides its users with built-in tools for managing their backends.</p> Command Description <code>thunder backend add</code> Add run config to the list of available configs. <code>thunder backend list</code> Show parameters of specified backend(s). <code>thunder backend remove</code> Delete backend from list. <code>thunder backend set</code> Set specified backend from list of available backends as default."},{"location":"cli/#examples","title":"Examples","text":""},{"location":"cli/#add","title":"add","text":"<p><pre><code>thunder backend add run_config_name backend=slurm ram=100 cpu=4 gpu=1 partition=partition_name\n</code></pre> If specified name already exists, you can use <code>--force</code> flag in order to overwrite it.  </p>"},{"location":"cli/#set","title":"set","text":"<pre><code>thunder backend set SOMENAME\nthunder backend list\n</code></pre>"},{"location":"cli/#list","title":"list","text":"<pre><code>thunder backend list NAME1 NAME2\n*shows backends with specified names*\n\nthunder backend list\n*shows all backends*\n</code></pre>"},{"location":"cli/#remove","title":"remove","text":"<pre><code>thunder backend remove SOMENAME\n</code></pre>"},{"location":"cli/#placeholders","title":"Placeholders","text":"<p>Some loggers and other tools in your experiment may require name  of the experiment. We find it convenient to use name of the folder you  build your experiment into as the name of the experiment for loggers.  Example with <code>WandbLogger</code>: <pre><code>from lightning.pytorch.loggers import WandbLogger\nfrom thunder.placeholders import ExpName, GroupName\n\nlogger = WandbLogger(name=ExpName, group=GroupName)\n</code></pre> In this case <code>GroupName</code> - name of the folder with built experiment and <code>ExpName</code> - name of the split.</p>"},{"location":"cli/#wandb-sweeps-integration","title":"WandB Sweeps integration","text":"<p>WandB has hyperparameters tuning system called Sweeps. Sweeps allow you to run multiple experiment with predefined grid of parameters and compare run results. However, we find default sweep execution system very inconvenient when it comes to running experiments on cluster.</p> <p>After running a few experiments with  WandB Logger,  you can create sweep configuration.  WandB will give a command <code>wandb agent project/sweep_id</code>. You can copy it and paste it into the following command: <pre><code>thunder PASTE_HERE /path/to/config /path/to/experiment \n</code></pre></p>"},{"location":"configs/","title":"Lazycon","text":"<p>Thunder embraces the power of lazycon allowing you to build configs for your  experiments. </p>"},{"location":"configs/#config-structure","title":"Config structure","text":"<p>Correct config should contain the following objects:</p> Name Required Description <code>trainer</code> Lightning Trainer instance. <code>module</code> LightningModule instance. <code>train_data</code> Loader of training data. <code>val_data</code> Loader of validation data. <code>test_data</code> Loader of test data. <code>predict_data</code> Loader of test data. <code>datamodule</code> LightningDataModule instance, replaces <code>train_data</code>, <code>val_data</code> and <code>test_data</code> if specified. <p>After executing <code>thunder run</code> (see Executing a config), thunder will extract  necessary fields. If some optional field (e.g. <code>val_data</code>) is not provided, features  dependent on it will not be used (e.g. no validation if <code>val_data</code> is not provided).</p>"},{"location":"configs/#executing-a-config","title":"Executing a config","text":"<p>Thunder has its own Command Line Interface,  about which you can read here.</p>"},{"location":"configs/#logging","title":"Logging","text":"<p>All primitive values (e.g. int, float, tuples) are logged automatically via config parsing.</p>"},{"location":"core/thunder_module/","title":"ThunderModule","text":"<p>ThunderModule inherits everything from LightningModule and implements essential methods for most common training pipelines.</p>"},{"location":"core/thunder_module/#from-lightning-to-thunder","title":"From Lightning to Thunder","text":"<p>Most common pipelines are implemented in lightning in the following way: <pre><code>from lightning import LightningModule\n\nclass Model(LightningModule):\n    def __init__(self):\n        self.architecture: nn.Module = ...\n        self.metrics = ... # smth like Dict[str, Callable]\n\n    def forward(self, *args, **kwargs):\n        return self.architecture(*args, **kwargs)\n\n    def criterion(self, x, y):\n        ...\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        return self.criterion(self(x), y)\n\n    def validation_step(self, batch, batch_idx, dataloader_idx):\n        # forward and metrics computation or output preservation\n        ...\n\n    def test_step(self, batch, batch_idx, dataloader_idx):\n        # forward and metrics computation or output preservation\n        ...\n\n    def configure_optimizers(self):\n        return Adam(...), StepLR(...)\n</code></pre></p> <p>ThunderModule offers an implementation of necessary steps shown above. <pre><code>from thunder import ThunderModule\n\narchitecture: nn.Module = ...\ncriterion = CrossEntropy()\noptimizer = Adam(architecture.parameters())\nscheduler = StepLR(optimizer)\n\nmodel = ThunderModule(architecture, criterion,\n                      optimizer=optimizer, lr_scheduler=scheduler)\n</code></pre></p>"},{"location":"core/thunder_module/#configuring-optimizers","title":"Configuring Optimizers","text":"<p>For extra information see this. Lightning requires optimizers and learning rate policies to be defined inside <code>configure_optimizers</code> method. Using ThunderModule allows you to pass the following configurations of  optimizers and learning rate schedulers:</p> <pre><code>from torch import nn\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom torch.optim import Adam\n\narchitecture = nn.Linear(2, 2)\n</code></pre>"},{"location":"core/thunder_module/#no-scheduling","title":"No scheduling","text":"<pre><code>optimizer = Adam(architecture.parameters())\nmodel = ThunderModule(..., optimizer=optimizer)\n</code></pre>"},{"location":"core/thunder_module/#defining-optimizer-and-scheduler","title":"Defining optimizer and scheduler","text":"<pre><code>optimizer = Adam(architecture.parameters())\nlr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., optimizer=optimizer, lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#defining-no-optimizer","title":"Defining no optimizer","text":"<pre><code>lr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#multiple-optimizers","title":"Multiple Optimizers","text":"<p>Thunder just as lightning supports configuration with more than 1 optimizer. If such configuration is to be used, manual optimization is required. Guide on manual optimization</p> <p>In thunder you can pass lists of optimizers and schedulers to ThunderModule. <pre><code>class ThunderModuleManual(ThunderModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.automatic_optimization = False\n\n\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Scheduler(opt) for opt in optimizers]\n\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p>"},{"location":"core/thunder_module/#thunder-policies","title":"Thunder Policies","text":"<p>As shown above, torch schedulers require optimizer(s) to be passed to them before they are given to ThunderModule. It is not very convenient, and also they lack some basic  functionality. You can use thunder policies just like torch schedulers: <pre><code>from thunder.policy import Switch\n\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Switch({1: 0.001}), Switch({2: 0.001})]\n\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p> <p>For extra information see Thunder Policies Docs.</p>"},{"location":"core/thunder_module/#inference","title":"Inference","text":"<p>During inference step, ThunderModule uses Predictors in order to preprocess data and make inverse transforms after passing data through the model. Default predictor is just an identity function.</p> <p>For more on predictors see Thunder Predictors Docs.</p>"},{"location":"core/thunder_module/#batch-transfer","title":"Batch Transfer","text":"<p>ThunderModule transfers training batches to device by default. However, during  inference batch remains on the device, on which it was received from data loader.  Transferring happens later in the <code>inference_step</code>, which is invoked in <code>validation_step</code>, <code>test_step</code> and <code>predict_step</code>.</p>"},{"location":"core/thunder_module/#reference","title":"Reference","text":""},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule","title":"<code>thunder.torch.core.ThunderModule</code>","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>thunder/torch/core.py</code> <pre><code>class ThunderModule(LightningModule):\n    def __init__(\n            self,\n            architecture: nn.Module,\n            criterion: Callable,\n            n_targets: int = 1,\n            activation: Callable = identity,\n            optimizer: Union[List[Optimizer], Optimizer] = None,\n            lr_scheduler: Union[List[LRScheduler], LRScheduler] = None,\n            predictor: BasePredictor = None,\n            n_val_targets: int = None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        architecture: nn.Module\n            Model architecture used to conduct forward pass.\n        criterion: Callable\n            Criterion to optimize.\n        n_targets: int\n            Number of target values in train and inference batches, if negative, then ...\n        activation: Callable\n            Final activation function for inference, identity by default.\n        optimizer: Union[List[Optimizer], Optimizer]\n            Optimizers.\n        lr_scheduler: Union[List[LRScheduler], LRScheduler]\n            Learning Rate policies.\n        predictor: BasePredictor.\n            Predictor for inference.\n        n_val_targets: int\n            Number of target values for inference, if set to None assumes value of `n_targets`.\n        \"\"\"\n        super().__init__()\n        self.architecture = architecture\n        self.criterion = criterion\n        self.n_targets = n_targets\n        self.n_val_targets = n_targets if n_val_targets is None else n_val_targets\n        self.activation = activation\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.predictor = predictor if predictor else Predictor()\n\n    def transfer_batch_to_device(self, batch: Tuple, device: torch.device, dataloader_idx: int) -&gt; Any:\n        if self.trainer.state.stage != \"train\":\n            return batch\n        return super().transfer_batch_to_device(maybe_from_np(batch, device=device), device, dataloader_idx)\n\n    def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n        return self.architecture(*args, **kwargs)\n\n    def training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT:\n        x, y = batch[: -self.n_targets], batch[-self.n_targets:]\n        return self.criterion(self(*x), *y)\n\n    def validation_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\n        return self.inference_step(batch, batch_idx, dataloader_idx)\n\n    def test_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\n        return self.inference_step(batch, batch_idx, dataloader_idx)\n\n    def predict_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\n        return self.inference_step(batch, batch_idx, dataloader_idx)\n\n    def predict(self, x) -&gt; STEP_OUTPUT:\n        # TODO: do we need super(). ...?, also consider changing maybe_to_np to smth stricter\n        x = maybe_from_np(x, device=self.device)\n        if not isinstance(x, (list, tuple)):\n            x = (x,)\n        return to_np(self.activation(self(*x)))\n\n    def inference_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\n        x, y = map(squeeze_first, (batch[:-self.n_val_targets], batch[-self.n_val_targets:]))\n        return self.predictor([x], self.predict)[0], y\n\n    def configure_optimizers(self) -&gt; Tuple[List[Optimizer], List[LRScheduler]]:\n        if not self.optimizer and not self.lr_scheduler:\n            raise NotImplementedError(\n                \"You must specify optimizer or lr_scheduler, \"\n                \"or implement configure_optimizers method\"\n            )\n\n        _optimizers = list(collapse([self.optimizer]))\n        _lr_schedulers = list(collapse([self.lr_scheduler]))\n        max_len = max(map(len, (_optimizers, _lr_schedulers)))\n        _optimizers = list(padded(_optimizers, None, max_len))\n        _lr_schedulers = list(padded(_lr_schedulers, None, max_len))\n\n        optimizers = []\n        lr_schedulers = []\n\n        for optimizer, lr_scheduler in zip_equal(_optimizers, _lr_schedulers):\n            if callable(lr_scheduler):\n                if optimizer is None:\n                    raise ValueError(\"The scheduler demands an Optimizer, but received None\")\n                lr_scheduler = lr_scheduler(optimizer)\n\n            optimizers.append(optimizer if lr_scheduler is None else lr_scheduler.optimizer)\n            if lr_scheduler is not None:\n                lr_schedulers.append(lr_scheduler)\n\n        if len(optimizers) &lt; len(lr_schedulers):\n            raise ValueError(\n                \"The number of optimizers must be greater or equal to the number of \"\n                f\"lr_schedulers, got {len(optimizers)} and {len(lr_schedulers)}\\n\"\n                f\"Optimizers: f{optimizers}\\n\"\n                f\"Schedulers: f{lr_schedulers}\\n\"\n            )\n\n        return optimizers, lr_schedulers\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT:\n    x, y = batch[: -self.n_targets], batch[-self.n_targets:]\n    return self.criterion(self(*x), *y)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def validation_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\n    return self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def test_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\n    return self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def predict_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\n    return self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.inference_step","title":"<code>inference_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def inference_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\n    x, y = map(squeeze_first, (batch[:-self.n_val_targets], batch[-self.n_val_targets:]))\n    return self.predictor([x], self.predict)[0], y\n</code></pre>"},{"location":"examples/","title":"Overview","text":"<p>Here are some examples of Thunder experiments.</p> Name Description Totalsegmentator LowRes Low resolution segmentation of liver in Totalsegmentator MNIST Classifier MNIST classifier config"},{"location":"examples/mnist/","title":"Mnist","text":"<pre><code>import numpy as np\nimport torch\nfrom lightning import Trainer\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import WandbLogger\nfrom sklearn.metrics import accuracy_score\nfrom thunder import ThunderModule\nfrom thunder.callbacks import MetricMonitor\nfrom thunder.placeholders import ExpName, GroupName\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nBATCH_SIZE = 256\n\ntrain_ds = MNIST(\".\", train=True, download=True, transform=transforms.ToTensor())\nval_ds = MNIST(\".\", train=False, download=True, transform=transforms.ToTensor())\ntrain_data = DataLoader(train_ds, batch_size=BATCH_SIZE)\nval_data = DataLoader(val_ds, batch_size=BATCH_SIZE)\n\narchitecture = nn.Sequential(nn.Flatten(), torch.nn.Linear(28 * 28, 10))\n\nmodule = ThunderModule(\n    architecture, nn.CrossEntropyLoss(), optimizer=torch.optim.Adam(architecture.parameters())\n)\n\n# Initialize a trainer\ntrainer = Trainer(\n    callbacks=[ModelCheckpoint(save_last=True),\n               MetricMonitor(group_metrics={lambda y, x: (np.argmax(y), x): accuracy_score})],\n    accelerator=\"auto\",\n    devices=1,\n    max_epochs=100,\n    logger=WandbLogger(\n        name=ExpName,\n        group=GroupName,\n        project=\"thunder-examples\",\n        entity=\"arseniybelkov\",\n    ),\n)\n</code></pre>"},{"location":"examples/mnist/#source","title":"Source","text":"<p>Full source code is available at thunder-examples</p>"},{"location":"examples/totalsegm_lowres/","title":"Low Resolution Liver Segmentation","text":""},{"location":"examples/totalsegm_lowres/#requirements-deep-pipe-amid-connectome","title":"Requirements: deep-pipe, amid, connectome","text":"<p>Deep-Pipe was primarly used for metrics and batch combinations.</p>"},{"location":"examples/totalsegm_lowres/#main-config","title":"Main config","text":"<pre><code>from functools import partial\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom amid.totalsegmentator import Totalsegmentator\nfrom connectome import Apply, CacheToDisk, CacheToRam, Chain, Filter\nfrom dpipe import layers\nfrom dpipe.batch_iter import combine_pad\nfrom dpipe.im.metrics import dice_score, precision, recall\nfrom dpipe.torch.functional import weighted_cross_entropy_with_logits\nfrom lightning import Trainer\nfrom lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom lightning.pytorch.loggers import WandbLogger\nfrom sklearn.model_selection import train_test_split\nfrom thunder import ThunderModule\nfrom thunder.callbacks import MetricMonitor, TimeProfiler\nfrom thunder.layout import SingleSplit\nfrom thunder.placeholders import ExpName, GroupName\nfrom thunder.policy import Switch\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\n\nfrom thunder_examples.dataset import (ConToTorch, NormalizeCT, RotateTotalsegm,\n                                      Zoom)\n\nSEED = 0xBadCafe\n\ntotalsegmentator = Totalsegmentator(\"/shared/data/Totalsegmentator_dataset.zip\")\n                   &gt;&gt; Filter(lambda study_type, split: study_type == \"ct abdomen-pelvis\" and split == \"train\")\npreprocessing = Chain(RotateTotalsegm(), Zoom(n=0.3), NormalizeCT(max_=200, min_=-200))\n\ndataset = Chain(totalsegmentator,\n                preprocessing,\n                CacheToRam())\n\nlayout = SingleSplit(dataset, train=0.7, val=0.3)\n\nbatch_size = 2\nbatches_per_epoch = 256\nmax_epochs = 200\n\ntrain_data = DataLoader(\n    ConToTorch(layout.train &gt;&gt; Apply(image=lambda x: x[None], liver=lambda x: x[None]), ['image', 'liver']),\n    batch_size=batch_size, num_workers=4,\n    shuffle=True, collate_fn=partial(combine_pad, padding_values=np.min))\n\nval_data = DataLoader(\n    ConToTorch(layout.val &gt;&gt; Apply(image=lambda x: x[None], liver=lambda x: x[None]), ['image', 'liver']),\n    batch_size=batch_size, collate_fn=partial(combine_pad, padding_values=np.min), num_workers=4)\n\narchitecture = nn.Sequential(\n    nn.Conv3d(1, 8, kernel_size=3, padding=1),\n\n    layers.FPN(\n        layers.ResBlock3d, nn.MaxPool3d(2), nn.Identity(),\n        layers.fpn.interpolate_merge(lambda x, y: torch.cat([x, y], 1), order=1),\n        [\n            [[8, 16, 16], [32, 16, 8]],\n            [[16, 32, 32], [64, 32, 16]],\n            [[32, 64, 64], [128, 64, 32]],\n            [[64, 128, 128], [256, 128, 64]],\n            [128, 256, 128],\n        ],\n        kernel_size=3, padding=1,\n    ),\n\n    layers.PreActivation3d(8, 1, kernel_size=3, padding=1),\n)\n\ncriterion = weighted_cross_entropy_with_logits\n\nmodule = ThunderModule(architecture, criterion, activation=nn.Sigmoid(),\n                       optimizer=Adam(architecture.parameters()),\n                       lr_scheduler=Switch({0: 1e-3, 50: 1e-4, 150: 1e-5}))\n\ntrainer = Trainer(\n    callbacks=[\n        MetricMonitor({lambda y, x: (y &gt; 0.5, x &gt; 0.5): [precision, recall, dice_score]},\n                      aggregate_fn=[\"std\", \"max\", \"min\"]),\n        TimeProfiler(),\n        LearningRateMonitor(\"epoch\"),\n        ModelCheckpoint(save_last=True),\n    ],\n    limit_train_batches=batches_per_epoch,\n    accelerator='gpu', precision=16,\n    max_epochs=max_epochs,\n    logger=WandbLogger(name=ExpName, group=GroupName, project='thunder-examples', entity='arseniybelkov'))\n</code></pre>"},{"location":"examples/totalsegm_lowres/#contotorch","title":"ConToTorch","text":"<p>ConTotch is a wrapper for connectome dataset for it can be passed to torch DataLoader. <pre><code>from torch.utils.data import Dataset\n\n\nclass ConToTorch(Dataset):\n    def __init__(self, dataset, fields):\n        self.loader = dataset._compile(fields)\n        self.ids = dataset.ids\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, item):\n        return self.loader(self.ids[item])\n</code></pre></p>"},{"location":"examples/totalsegm_lowres/#source","title":"Source","text":"<p>Full source code is available at thunder-examples</p>"},{"location":"inference/","title":"Inference","text":"<p>For matters of inference, thunder offers Predictors - objects that  can transform data before and after model's inference.</p> <p>In ThunderModule Predictor is used as default inference runner.</p>"},{"location":"inference/#predictors","title":"Predictors","text":""},{"location":"inference/#thunder.predict.predict","title":"<code>thunder.predict.predict</code>","text":""},{"location":"inference/#thunder.predict.predict.BasePredictor","title":"<code>BasePredictor</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for all predictors.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class BasePredictor(ABC):\n    \"\"\"Base class for all predictors.\"\"\"\n\n    @abstractmethod\n    def forward(self, batches: Iterable) -&gt; Iterable:\n        \"\"\"Process stream of batches before model inference.\"\"\"\n        raise NotImplementedError(\"You must implement forward method\")\n\n    @abstractmethod\n    def backward(self, predicts: Iterable) -&gt; Iterable:\n        \"\"\"Post-process stream of predictions.\"\"\"\n        raise NotImplementedError(\"You must implement backward method\")\n\n    def __call__(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n        return self.run(batches, predict_fn)\n\n    def run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n        \"\"\"Runs preprocessing, inference and postprocessing.\"\"\"\n        return self.backward(map(predict_fn, self.forward(batches)))\n</code></pre>"},{"location":"inference/#thunder.predict.predict.BasePredictor.backward","title":"<code>backward(predicts)</code>  <code>abstractmethod</code>","text":"<p>Post-process stream of predictions.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>@abstractmethod\ndef backward(self, predicts: Iterable) -&gt; Iterable:\n    \"\"\"Post-process stream of predictions.\"\"\"\n    raise NotImplementedError(\"You must implement backward method\")\n</code></pre>"},{"location":"inference/#thunder.predict.predict.BasePredictor.forward","title":"<code>forward(batches)</code>  <code>abstractmethod</code>","text":"<p>Process stream of batches before model inference.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>@abstractmethod\ndef forward(self, batches: Iterable) -&gt; Iterable:\n    \"\"\"Process stream of batches before model inference.\"\"\"\n    raise NotImplementedError(\"You must implement forward method\")\n</code></pre>"},{"location":"inference/#thunder.predict.predict.BasePredictor.run","title":"<code>run(batches, predict_fn)</code>","text":"<p>Runs preprocessing, inference and postprocessing.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>def run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n    \"\"\"Runs preprocessing, inference and postprocessing.\"\"\"\n    return self.backward(map(predict_fn, self.forward(batches)))\n</code></pre>"},{"location":"inference/#thunder.predict.predict.Decorated","title":"<code>Decorated</code>","text":"<p>             Bases: <code>Predictor</code></p> <p>Decorates inference function Example</p> <p>Decorated(f, g, h)</p>"},{"location":"inference/#thunder.predict.predict.Decorated--inside-decorated","title":"inside Decorated","text":"<p>predict_fn = f(g(h(predict_fn)))</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class Decorated(Predictor):\n    \"\"\"\n    Decorates inference function\n    Example\n    -----------\n    Decorated(f, g, h)\n    # inside Decorated\n    predict_fn = f(g(h(predict_fn)))\n    \"\"\"\n\n    def __init__(self, *decorators: Callable):\n        self.decorators = compose(*decorators)\n\n    def run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n        return super().run(batches, self.decorators(predict_fn))\n</code></pre>"},{"location":"inference/#thunder.predict.predict.InfinitePredictor","title":"<code>InfinitePredictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> <p>Useful for running inference on infinite stream of data.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class InfinitePredictor(BasePredictor):\n    \"\"\"Useful for running inference on infinite stream of data.\"\"\"\n\n    def forward(self, batches: Iterable) -&gt; Iterable:\n        yield from batches\n\n    def backward(self, predicts: Iterable) -&gt; Iterable:\n        yield from predicts\n</code></pre>"},{"location":"inference/#thunder.predict.predict.Predictor","title":"<code>Predictor</code>","text":"<p>             Bases: <code>InfinitePredictor</code></p> <p>Assumes using finite amount of data for inference to be run on.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class Predictor(InfinitePredictor):\n    \"\"\"Assumes using finite amount of data for inference to be run on.\"\"\"\n\n    def run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n        return tuple(super().run(batches, predict_fn))\n</code></pre>"},{"location":"layout/","title":"Layout","text":"<p>Layout instances are responsible for splitting your datasets and managing which data fold is used for each experiment. They also check reproducibility of your data splits.</p>"},{"location":"layout/#usage","title":"Usage","text":"<p>Layout is a cornerstone of every built experiment. Via layout one can define structure of  their data splits.  </p> <p>After building an experiment one may want to extract data of created splits.  Heretofore it was done by manually loading the ids. Thunder's Layouts provides interface for extracting them by simply loading experiment config.  <pre><code>import lazycon\ncfg = lazycon.load(\"/path/to/experiment.config\")\ncfg.layout.set(fold=0)\ntrain_ids = cfg.layout.train # cfg.layout.SPLIT_NAME \n</code></pre></p>"},{"location":"layout/#thunder-layouts","title":"Thunder Layouts","text":"Name Description Split Layout for K fold cross-validation SingleSplit Layout with several sets (e.g. train + val + test) FixedSplit Creates layout from predefined K-fold   split FixedSingleSplit Creates single fold layout from predefined data split. <p>All Layout subclasses follow common interface </p>"},{"location":"layout/#thunder.layout.interface.Layout","title":"<code>thunder.layout.interface.Layout</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>thunder/layout/interface.py</code> <pre><code>class Layout(ABC):\n    @abstractmethod\n    def build(self, experiment: Path, config: Config) -&gt; Iterable[Node]:\n        pass\n\n    @abstractmethod\n    def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\n        pass\n\n    @abstractmethod\n    def set(self, **kwargs):\n        pass\n</code></pre>"},{"location":"layout/fixed/","title":"Fixed Splits","text":"<p>This feature is currently not stable, changes may be applied in the future.</p> <p>Fixed splits allow to create layouts from predefined splits of data.</p>"},{"location":"layout/fixed/#thunder.layout.fixed.FixedSplit","title":"<code>thunder.layout.fixed.FixedSplit</code>","text":"<p>             Bases: <code>Split</code></p> Source code in <code>thunder/layout/fixed.py</code> <pre><code>class FixedSplit(Split):\n    def __init__(self, splits: Sequence, *names: str):\n        \"\"\"\n        Creates experiment layout from given split.\n        Parameters\n        ----------\n        splits: Sequence\n            Split of data.\n        *names: str\n            Names of folds, e.g. 'train', 'val', test'.\n        Examples\n        ----------\n        ```python\n        # 3 folds of train-val splits.\n        split: list = [[[...], [...]],\n                        [[...], [...]],\n                        [[...], [...]]]\n        layout = FixedSplit(split, \"train\", \"val\")\n        ```\n        \"\"\"\n        if names:\n            if len(set(names)) != len(names):\n                raise ValueError(f\"Names of splits are not unique: {names}\")\n            if len(splits[0]) != len(names):\n                raise ValueError(f\"Got {len(splits[0])} and {len(names)} fold names: {names}\")\n\n        self.entries = sorted(set(collapse(splits)))\n        self.splits = [tuple(fold) for fold in splits]\n        self.names = names\n        self.fold: Optional[int] = None\n</code></pre>"},{"location":"layout/fixed/#thunder.layout.fixed.FixedSingleSplit","title":"<code>thunder.layout.fixed.FixedSingleSplit</code>","text":"<p>             Bases: <code>SingleSplit</code></p> Source code in <code>thunder/layout/fixed.py</code> <pre><code>class FixedSingleSplit(SingleSplit):\n    def __init__(self, split: Union[Sequence, Dict[str, Sequence]], *names: str):\n        \"\"\"\n        Creates single fold experiment from given split.\n        Parameters\n        ----------\n        split: Union[Sequence, Dict[str, Sequence]]\n            split of data\n        *names: str\n            Names of folds, e.g. 'train', 'val', test'. If data is of type `dict`,\n            then it is not required.\n        Examples\n        ----------\n        ```python\n        split: dict = {\"train\": [...], \"val\": [...]}\n        layout = FixedSingleSplit(split)\n        # or\n        split: list = [[...], [...]]\n        layout = FixedSingeSplit(split, \"train\", \"val\")\n        ```\n        \"\"\"\n        if isinstance(split, dict):\n            names = split.keys()  # from python3.7 order is guaranteed.\n            split = split.values()\n\n        if len(names) != len(split):\n            raise ValueError(\"Difference in number of splits and number of names: \"\n                             f\"{len(split)} and {len(names)}\")\n\n        self.split = dict(zip_equal(names, split))\n        self.entries = sorted(collapse(split))\n</code></pre>"},{"location":"layout/splits/","title":"Splits","text":""},{"location":"layout/splits/#thunder.layout.split.Split","title":"<code>thunder.layout.split.Split</code>","text":"<p>             Bases: <code>Layout</code></p> Source code in <code>thunder/layout/split.py</code> <pre><code>class Split(Layout):\n    def __init__(self, split: SplitType, entries: Sequence, *args: Any, names: Optional[Sequence[str]] = None,\n                 **kwargs: Any):\n        \"\"\"\n        Splits data according to split function.\n        Parameters\n        ----------\n        split: Callable\n            Split function, or a sklearn splitter.\n        entries: Sequence\n            Series of ids or torch Dataset or Connectome Layer.\n        args: Any\n            args for split.\n        names: Optional[Sequence[str]]\n            Names of folds, e.g. 'train', 'val', test'\n        kwargs: Any\n            kwargs for split.\n        Examples\n        ----------\n        ```python\n        from sklearn.model_selection import KFold\n\n        ids = [0, 1, ...]\n        layout = Split(KFold(3), ids, names=[\"train\", \"test\"])\n        ```\n        \"\"\"\n        if not callable(split):\n            if not hasattr(split, 'split'):\n                raise TypeError(f'Expected either a function, or a sklearn splitter, got {type(split)!r}')\n            split = split.split\n\n        ids = entries_to_ids(entries)\n        # TODO: safer way to unify types\n        splits = [tuple(map(jsonify, xs)) for xs in split(ids, *args, **kwargs)]\n        if names is not None:\n            # TODO\n            assert len(set(names)) == len(names)\n            assert len(splits[0]) == len(names)\n\n        self.entries = entries\n        self.splits = splits\n        self.names = names\n        self.fold: Optional[int] = None\n\n    def __getitem__(self, item: int):\n        return self._subset(item)\n\n    def __getattr__(self, name: str):\n        if self.names is None:\n            raise AttributeError(name)\n        return self._subset(self.names.index(name))\n\n    def _subset(self, idx):\n        # TODO\n        assert self.fold is not None\n        return entries_subset(self.entries, self.splits[self.fold][idx])\n\n    def build(self, experiment: Path, config: Config):\n        config.dump(experiment / 'experiment.config')\n        name = experiment.name\n        for fold, split in enumerate(self.splits):\n            folder = experiment / f'fold_{fold}'\n            folder.mkdir()\n            save(split, folder / 'split.json')\n\n            local = config.copy().update(ExpName=f'{name}({fold})', GroupName=name)\n            local.dump(folder / 'experiment.config')\n            yield Node(name=str(fold))\n\n    def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\n        folder = experiment / f'fold_{node.name}'\n        return Config.load(folder / 'experiment.config'), folder, {\n            'fold': int(node.name),\n            'split': tuple(load(folder / 'split.json')),\n        }\n\n    def set(self, fold: int, split: Optional[Sequence[Sequence]] = None):\n        self.fold = fold\n        if split is None:\n            warnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\n        else:\n            if split != self.splits[fold]:\n                # TODO: consistency error?\n                raise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.build","title":"<code>build(experiment, config)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def build(self, experiment: Path, config: Config):\n    config.dump(experiment / 'experiment.config')\n    name = experiment.name\n    for fold, split in enumerate(self.splits):\n        folder = experiment / f'fold_{fold}'\n        folder.mkdir()\n        save(split, folder / 'split.json')\n\n        local = config.copy().update(ExpName=f'{name}({fold})', GroupName=name)\n        local.dump(folder / 'experiment.config')\n        yield Node(name=str(fold))\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.load","title":"<code>load(experiment, node)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\n    folder = experiment / f'fold_{node.name}'\n    return Config.load(folder / 'experiment.config'), folder, {\n        'fold': int(node.name),\n        'split': tuple(load(folder / 'split.json')),\n    }\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.set","title":"<code>set(fold, split=None)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def set(self, fold: int, split: Optional[Sequence[Sequence]] = None):\n    self.fold = fold\n    if split is None:\n        warnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\n    else:\n        if split != self.splits[fold]:\n            # TODO: consistency error?\n            raise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit","title":"<code>thunder.layout.split.SingleSplit</code>","text":"<p>             Bases: <code>Layout</code></p> Source code in <code>thunder/layout/split.py</code> <pre><code>class SingleSplit(Layout):\n    def __init__(self, entries: Sequence, *, shuffle: bool = True,\n                 random_state: Union[np.random.RandomState, int, None] = 0,\n                 **sizes: Union[int, float]):\n        \"\"\"\n        Creates single fold experiment, with custom number of sets.\n        Parameters\n        ----------\n        entries: Sequence\n            Sequence of ids or\n        shuffle: bool\n            Whether to shuffle entries.\n        random_state : Union[np.random.RandomState, int, None]\n        sizes: Union[int, float]\n            Size of each split.\n        Examples\n        ----------\n        ```python\n        ids = [...]\n        layout = SingleSplit(ids, train=0.7, val=0.1, test=0.2)\n        ```\n        \"\"\"\n        if not isinstance(random_state, np.random.RandomState):\n            random_state = np.random.RandomState(random_state)\n\n        ids = entries_to_ids(entries)\n        self.entries = entries\n        self.split = dict(zip(sizes.keys(), multi_split(\n            ids, list(sizes.values()), shuffle=shuffle, random_state=random_state\n        )))\n\n    def __getattr__(self, name: str):\n        if name not in self.split:\n            raise AttributeError(name)\n        return entries_subset(self.entries, self.split[name])\n\n    def build(self, experiment: Path, config: Config):\n        config.dump(experiment / 'experiment.config')\n        name = experiment.name\n        save(self.split, experiment / 'split.json')\n\n        local = config.copy().update(ExpName=name, GroupName=name)\n        local.dump(experiment / 'experiment.config')\n        return []\n\n    def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\n        return Config.load(experiment / 'experiment.config'), experiment, {\n            'split': load(experiment / 'split.json'),\n        }\n\n    def set(self, split: Optional[Dict[str, Sequence]] = None):\n        if split is None:\n            warnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\n        else:\n            if split != self.split:\n                # TODO: consistency error?\n                raise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.build","title":"<code>build(experiment, config)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def build(self, experiment: Path, config: Config):\n    config.dump(experiment / 'experiment.config')\n    name = experiment.name\n    save(self.split, experiment / 'split.json')\n\n    local = config.copy().update(ExpName=name, GroupName=name)\n    local.dump(experiment / 'experiment.config')\n    return []\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.load","title":"<code>load(experiment, node)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\n    return Config.load(experiment / 'experiment.config'), experiment, {\n        'split': load(experiment / 'split.json'),\n    }\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.set","title":"<code>set(split=None)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def set(self, split: Optional[Dict[str, Sequence]] = None):\n    if split is None:\n        warnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\n    else:\n        if split != self.split:\n            # TODO: consistency error?\n            raise ValueError\n</code></pre>"},{"location":"loggers/","title":"Loggers","text":"<p>A few loggers provided were modified in order to increase their convenience for configs.  If you use thunder configs (see Configs), all primitive values (e.g. int, float, tuples) are logged automatically via config parsing. </p> Name Description WandbLogger Can remove duplicated experiments."},{"location":"loggers/wandb/","title":"WandbLogger","text":"<p>Slightly modified WandbLogger. <code>thunder</code> version has additional parameter <code>remove_dead_duplicates</code> and <code>allow_rerun</code>  that if being set to <code>True</code> (<code>False</code> by default), deletes all crashed or failed runs  with the same name and group within your project. <pre><code>from thunder.torch.loggers import WandbLogger\nlogger = WandbLogger(..., remove_dead_duplicates=True)\n</code></pre></p> <p><code>allow_rerun</code> equals <code>True</code> disables <code>remove_dead_duplicates</code> and seeks last experiment WandB has created in your experiment folder.  If your experiment failed during the previous run and you restarted it with <code>allow_rerun = True</code>. WandB will not create new versions, but will use the last run for logging and checkpointing.</p>"},{"location":"policy/","title":"Policy","text":"<p>Policies are objects that define how some value changes through time. Good example of them is Learning Rate Schedulers.  </p>"},{"location":"policy/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Contrary to default PyTorch Learning Rate schedulers, ours does not require an optimizer to be passed during initialization.</p>"},{"location":"policy/#thunder-schedulers","title":"Thunder Schedulers","text":"Name Description Multiply Multiplies lr on each step by specified factor. Schedule Assigns lr values according to specified callable. Switch Assigns lr values according to specified dict schedule. <p>See Learning Rate Schedulers docs </p>"},{"location":"policy/lr_schedulers/","title":"LR Schedulers","text":"<p>All schedulers in thunder are subclasses of <code>torch.optim.lr_scheduler.LRScheduler</code>. However, during initialization they do not require optimizer to be passed.</p>"},{"location":"policy/lr_schedulers/#usage","title":"Usage","text":"<p>We will use Switch as an example. <pre><code>from thunder.policy import Switch\n\nswitch = Switch({10: 0.001, 20: 0.001 / 10})\n</code></pre> We have just created a policy, but to make it work, it still needs an optimizers. Let's see how it works after being assembled. <pre><code>optimizer = Adam(...)\nscheduler(optimizer) # binds optimizer to scheduler\n# or \n# scheduler = scheduler(optimizer)\n# You can also retrieve optimizer:\nopt = scheduler.optimizer\n</code></pre> After assigning optimizer to scheduler, policy instance will work just like usual torch scheduler.</p>"},{"location":"policy/lr_schedulers/#initial-lr","title":"Initial LR","text":"<p>All schedulers have <code>lr_init</code> parameters, if specified, it will be used as lr value on 0th step.</p>"},{"location":"policy/lr_schedulers/#reference","title":"Reference","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Multiply","title":"<code>thunder.policy.Multiply</code>","text":"<p>             Bases: <code>MappingPolicy</code></p> <p>Multiplies learning rate value on the specified factor in <code>mapping</code>. Example:     <pre><code>    sch = Multiply({1: 0.1, 4: 0.3})\n</code></pre>     if initial learning rate is 1e-3, learning rate will be: 1e-3, 1e-4, 1e-4, 1e-4, 3-e5, ...</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply--parameters","title":"Parameters","text":"<p>mapping: Union[List[Dict[int, float]], Dict[int, float]]     Maps epoch to factor, keeping the last value between the epochs. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Multiply(MappingPolicy):\n    \"\"\"\n    Multiplies learning rate value on the specified factor in `mapping`.\n    Example:\n        ```python\n            sch = Multiply({1: 0.1, 4: 0.3})\n        ```\n        if initial learning rate is 1e-3, learning rate will be: 1e-3, 1e-4, 1e-4, 1e-4, 3-e5, ...\n\n    Parameters\n    ----------\n    mapping: Union[List[Dict[int, float]], Dict[int, float]]\n        Maps epoch to factor, keeping the last value between the epochs.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\n    mapping: Union[List[Dict[int, float]], Dict[int, float]]\n\n    def get_lr(self) -&gt; List[float]:\n        return [\n            param_group[\"lr\"] * mapping.get(self.last_epoch, 1)\n            for param_group, mapping in zip_equal(self.optimizer.param_groups, self.current_mapping)\n        ]\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        return super().state_dict()\n\n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule","title":"<code>thunder.policy.Schedule</code>","text":"<p>             Bases: <code>MappingPolicy</code></p> <p>Assigns learning rate values received from callable mapping. Example:     <pre><code>sch = Schedule(np.cos)\n</code></pre>     lr will have values of np.cos(epoch_number)</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule--parameters","title":"Parameters","text":"<p>mapping: Union[List[Callable], Callable]]     Maps epoch to value. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Schedule(MappingPolicy):\n    \"\"\"\n    Assigns learning rate values received from callable mapping.\n    Example:\n        ```python\n        sch = Schedule(np.cos)\n        ```\n        lr will have values of np.cos(epoch_number)\n\n    Parameters\n    ----------\n    mapping: Union[List[Callable], Callable]]\n        Maps epoch to value.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\n    mapping: Union[List[Callable], Callable]\n\n    def get_lr(self) -&gt; List[float]:\n        return juxt(self.current_mapping)(self.last_epoch)\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        return super().state_dict(\"mapping\", \"current_mapping\")\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch","title":"<code>thunder.policy.Switch</code>","text":"<p>             Bases: <code>MappingPolicy</code></p> <p>Assigns learning rate values received from dict mapping. Example:     <pre><code>sch = Switch({0: 1e-4, 2: 1e-10)\n</code></pre>     lr: 1e-4, 1e-4, 1e-10, 1e-10, ...</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch--parameters","title":"Parameters","text":"<p>mapping: Union[List[Dict[int, float]], Dict[int, float]]     Maps specified epochs to specified values, preserving learning rate between epochs. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Switch(MappingPolicy):\n    \"\"\"\n    Assigns learning rate values received from dict mapping.\n    Example:\n        ```python\n        sch = Switch({0: 1e-4, 2: 1e-10)\n        ```\n        lr: 1e-4, 1e-4, 1e-10, 1e-10, ...\n\n    Parameters\n    ----------\n    mapping: Union[List[Dict[int, float]], Dict[int, float]]\n        Maps specified epochs to specified values, preserving learning rate between epochs.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\n    mapping: Union[List[Dict[int, float]], Dict[int, float]]\n\n    def get_lr(self) -&gt; List[float]:\n        return [\n            mapping.get(self.last_epoch, param_group[\"lr\"])\n            for param_group, mapping in zip_equal(self.optimizer.param_groups, self.current_mapping)\n        ]\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        return super().state_dict()\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#base-classes","title":"Base classes","text":""},{"location":"policy/lr_schedulers/#thunder.policy","title":"<code>thunder.policy</code>","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Policy","title":"<code>Policy</code>","text":"<p>             Bases: <code>_LRScheduler</code></p> <p>Policy base class.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Policy(LRScheduler, metaclass=ABCMeta):\n    \"\"\"\n    Policy base class.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def __call__(self, optimizer: Optimizer) -&gt; Policy:\n        self.set_optimizer(optimizer)\n        return self\n\n    def set_optimizer(self, optimizer: Optimizer) -&gt; None:\n        \"\"\"Assigns optimizer to a scheduler\"\"\"\n        super().__init__(optimizer)\n\n    @abstractmethod\n    def get_lr(self) -&gt; List[float]:\n        \"\"\"\n        Computes new value of learning rate.\n        Returns\n        -------\n        List[float]\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def state_dict(self, *keys: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Creates state dict of scheduler, excluding optimizer.\n        Parameters\n        ----------\n        keys: str\n            Names of attributes to be excluded from state_dict\n\n        Returns\n        -------\n        Dict[str, Any]\n        \"\"\"\n        keys = (*keys, \"optimizer\")\n        return {key: value for key, value in self.__dict__.items() if key not in keys}\n\n    @abstractmethod\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Loads state dict of scheduler\n        Parameters\n        ----------\n        state_dict: Dict[str, Any]\n            State dict of scheduler.\n        \"\"\"\n        self.__dict__.update(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.get_lr","title":"<code>get_lr()</code>  <code>abstractmethod</code>","text":"<p>Computes new value of learning rate. Returns</p> <p>List[float]</p> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef get_lr(self) -&gt; List[float]:\n    \"\"\"\n    Computes new value of learning rate.\n    Returns\n    -------\n    List[float]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.load_state_dict","title":"<code>load_state_dict(state_dict)</code>  <code>abstractmethod</code>","text":"<p>Loads state dict of scheduler Parameters</p> <p>state_dict: Dict[str, Any]     State dict of scheduler.</p> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Loads state dict of scheduler\n    Parameters\n    ----------\n    state_dict: Dict[str, Any]\n        State dict of scheduler.\n    \"\"\"\n    self.__dict__.update(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.set_optimizer","title":"<code>set_optimizer(optimizer)</code>","text":"<p>Assigns optimizer to a scheduler</p> Source code in <code>thunder/policy.py</code> <pre><code>def set_optimizer(self, optimizer: Optimizer) -&gt; None:\n    \"\"\"Assigns optimizer to a scheduler\"\"\"\n    super().__init__(optimizer)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.state_dict","title":"<code>state_dict(*keys)</code>  <code>abstractmethod</code>","text":"<p>Creates state dict of scheduler, excluding optimizer. Parameters</p> <p>keys: str     Names of attributes to be excluded from state_dict</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.state_dict--returns","title":"Returns","text":"<p>Dict[str, Any]</p> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef state_dict(self, *keys: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Creates state dict of scheduler, excluding optimizer.\n    Parameters\n    ----------\n    keys: str\n        Names of attributes to be excluded from state_dict\n\n    Returns\n    -------\n    Dict[str, Any]\n    \"\"\"\n    keys = (*keys, \"optimizer\")\n    return {key: value for key, value in self.__dict__.items() if key not in keys}\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy","title":"<code>MappingPolicy</code>","text":"<p>             Bases: <code>Policy</code></p> Source code in <code>thunder/policy.py</code> <pre><code>class MappingPolicy(Policy, metaclass=ABCMeta):\n    def __init__(self, mapping, lr_init: Union[List[float], float] = 1e-3):\n        \"\"\"\n        Base class for policy with mapping. Mapping can be a dict or a function\n        (it should also be a list of latter types in case of multiple param groups).\n        Mapping is the binding between epoch or step number and learning rate value.\n        Parameters\n        ----------\n        mapping\n            Binding of epoch or step number and learning rate.\n        lr_init: Union[List[float], float]]\n            Initial learning rate for each group of parameters.\n        \"\"\"\n        self.current_mapping = None\n        self.mapping = mapping\n\n        self.current_lr_init = None\n        self.lr_init = lr_init\n\n        super().__init__()\n\n    def set_optimizer(self, optimizer: Optimizer) -&gt; None:\n        self.current_mapping = self.mapping\n        if isinstance(self.mapping, dict) or callable(self.mapping):\n            self.current_mapping = [deepcopy(self.mapping) for _ in optimizer.param_groups]\n\n        self.current_lr_init = self.lr_init\n        if isinstance(self.lr_init, (float, int)):\n            self.current_lr_init = [self.lr_init for _ in optimizer.param_groups]\n\n        if len(self.current_mapping) != len(optimizer.param_groups):\n            raise ValueError(f\"Got {len(self.current_mapping)} mappings and {len(optimizer.param_groups)} param groups\")\n\n        if len(self.current_lr_init) != len(optimizer.param_groups):\n            raise ValueError(f\"Got {len(self.current_lr_init)} lr_init and {len(optimizer.param_groups)} param groups\")\n\n        for lr_init, param_group in zip(self.current_lr_init, optimizer.param_groups):\n            param_group[\"lr\"] = lr_init\n\n        super().set_optimizer(optimizer)\n\n    def __repr__(self) -&gt; str:\n        mapping = self.current_mapping if self.current_mapping else self.mapping\n        lr_init = self.current_lr_init if self.current_lr_init is not None else self.lr_init\n        return f\"{self.__class__.__name__}({mapping=}, {lr_init=})\"\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.__init__","title":"<code>__init__(mapping, lr_init=0.001)</code>","text":"<p>Base class for policy with mapping. Mapping can be a dict or a function (it should also be a list of latter types in case of multiple param groups). Mapping is the binding between epoch or step number and learning rate value. Parameters</p> <p>mapping     Binding of epoch or step number and learning rate. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>def __init__(self, mapping, lr_init: Union[List[float], float] = 1e-3):\n    \"\"\"\n    Base class for policy with mapping. Mapping can be a dict or a function\n    (it should also be a list of latter types in case of multiple param groups).\n    Mapping is the binding between epoch or step number and learning rate value.\n    Parameters\n    ----------\n    mapping\n        Binding of epoch or step number and learning rate.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\n    self.current_mapping = None\n    self.mapping = mapping\n\n    self.current_lr_init = None\n    self.lr_init = lr_init\n\n    super().__init__()\n</code></pre>"}]}