{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>You saw the lightning. Now it's time to hear the thunder</p>"},{"location":"#thunder","title":"Thunder","text":"<p>The Deep Learning framework based on Lighnting</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install from pypi <pre><code>pip install thunder\n</code></pre> or directly from GitHub <pre><code>git clone https://github.com/neuro-ml/thunder.git\ncd thunder &amp;&amp; pip install -e .\n</code></pre></p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#thundermodule","title":"ThunderModule","text":""},{"location":"#metriclogger","title":"MetricLogger","text":""},{"location":"#experiment-configs","title":"Experiment configs","text":""},{"location":"#cli-integrations-with-wandb","title":"CLI &amp; Integrations with WandB","text":""},{"location":"callbacks/","title":"Callbacks","text":"<p>Lightning Callbacks allow you to modify your training pipelines. For extra information see this.</p>"},{"location":"callbacks/#thunder-callbacks","title":"Thunder Callbacks","text":"Name Description MetricLogger Computes metrics and logs them TimeProfiler Logs the time of each LightningModule's step"},{"location":"callbacks/metric_logger/","title":"MetricLogger","text":"<p>This callback takes on computation and aggregation of the specified metrics.  </p>"},{"location":"callbacks/metric_logger/#usage","title":"Usage","text":""},{"location":"callbacks/metric_logger/#group-metrics","title":"Group metrics","text":"<p>Group metrics are computed on the entire dataset. For example, you want to compute classification accuracy on MNIST. <pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(group_metrics={\"accuracy\": accuracy_score})])\n</code></pre></p> <p>If you use any loggers (e.g. <code>Tensorboard</code> or <code>WandB</code>), <code>accuracy</code> will appear in them as follows: <code>val/accuracy</code> - validation metrics. <code>test/accuracy</code> - test metrics.</p>"},{"location":"callbacks/metric_logger/#single-metrics","title":"Single metrics","text":"<p>Single metrics are computed on each object separately and only then aggregated. It is a common use case for tasks like segmentation or object detection.</p>"},{"location":"callbacks/metric_logger/#simple-use-case","title":"Simple use case","text":"<p><pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(single_metrics={\"accuracy\": accuracy_score})])\n</code></pre> MetricLogger will log mean values by default. But you can add custom aggregations as well.</p>"},{"location":"callbacks/metric_logger/#custom-aggregations","title":"Custom aggregations","text":"<p>Let see what can be done if we want to log <code>std</code> of metrics as well as mean values. <pre><code>import numpy as np\nfrom thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\naggregate_fn = np.std\nmetric_logger = MetricLogger(single_metrics={\"accuracy\": accuracy_score},\naggregate_fn=aggregate_fn) \ntrainer = Trainer(callbacks=[metric_logger])\n</code></pre> The mean values appera in loggers with no additional keys.  MetricCallback will try to infer the name of an aggregating function and use it as an additional key.</p> <p><code>val/accuracy</code> - validation mean accuracy. <code>val/std/accuracy</code> - validation accuracy std. <code>test/accuracy</code> - test mean accuracy. <code>test/std/accuracy</code> - test accuracy std.</p> <p><code>aggregate_fn</code> can also be specified as follows:</p> <p><pre><code>import numpy as np\naggregate_fn = [np.std, np.median]\naggregate_fn = [np.std, \"median\", \"max\", \"min\"]\naggregate_fn = {\"zero\": lambda x: x[0]}\n</code></pre> MetricLogger can accept <code>str</code> or <code>List[str]</code> as <code>aggregate_fn</code>,  in this format it supports the following metrics:</p> Name Function \"median\" <code>np.median</code> \"min\" <code>np.min</code> \"max\" <code>np.max</code> \"std\" <code>np.std</code>"},{"location":"callbacks/metric_logger/#preprocessing","title":"Preprocessing","text":"<p>Sometimes metrics require some preprocessing. In this case, keys of <code>single_metrics</code> dict must be callable objects. <pre><code>from sklearn.metrics import accuracy_score, recall_score\nthreshold = lambda x, y: (x &gt; 0.5, y)\nsingle_metrics = {threshold: [accuracy_score, recall_score()]} \n# or\nsingle_metrics = {threshold: {\"acc\": accuracy_score, \"rec\": recall_score}}\n# or\nsingle_metrics = {threshold: recall_score}\n...\n</code></pre></p>"},{"location":"callbacks/metric_logger/#reference","title":"Reference","text":""},{"location":"callbacks/metric_logger/#thunder.callbacks.metric_logger.MetricLogger","title":"thunder.callbacks.metric_logger.MetricLogger","text":"<pre><code>thunder.callbacks.metric_logger.MetricLogger(single_metrics: Dict = None, group_metrics: Dict[str, Callable] = None, aggregate_fn: Union[Dict[str, Callable], str, Callable, List[Union[str, Callable]]] = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>single_metrics</code> <code>Dict</code> <p>Metrics that are calculated on each object separately and then aggregated.</p> <code>None</code> <code>group_metrics</code> <code>Dict[str, Callable]</code> <p>Metrics that are calculated on entire dataset.</p> <code>None</code> <code>aggregate_fn</code> <code>Union[Dict[str, Callable], str, Callable, List[Union[str, Callable]]]</code> <p>How to aggregate metrics. By default computes mean value. If yoy specify somethind, then the callback will compute mean and the specified values.</p> <code>None</code>"},{"location":"callbacks/time_profiler/","title":"Time profiler","text":""},{"location":"callbacks/time_profiler/#timeprofiler","title":"TimeProfiler","text":"<p>Lightning Callback which allows you to measure the time each step takes and log it during the training process.</p>"},{"location":"callbacks/time_profiler/#logged-values","title":"Logged values","text":"<p>TimeProfiler logs the following steps:</p> Name Logged by default Description train batch Time taken by forward, optimizer step, and backward during train step. validation batch Time taken by forward during validation step. train epoch Time taken by train epoch without validation. validation epoch Time taken by validation epoch. avg train downtime* Average downtime in training step. avg val downtime Average downtime in validation step. backward Time taken by backprop. optimizer step Time taken by optimizer. total train downtime Total downtime in training epoch. total val downtime Total downtime in validation epoch. <p>*Downtime - the process during which model does not work (e.g. data loader is working now)  </p>"},{"location":"callbacks/time_profiler/#usage","title":"Usage","text":"<pre><code>from thunder.callbacks import TimeProfiler\nfrom lightning import Trainer\n# logs default keys and in addition backward and optimizer step\ntrainer = Trainer(callbacks=[TimeProfiler(\"backward\", \"optimizer step\")])\n</code></pre>"},{"location":"callbacks/time_profiler/#reference","title":"Reference","text":""},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler","title":"thunder.callbacks.time_profiler.TimeProfiler","text":"<pre><code>thunder.callbacks.time_profiler.TimeProfiler(*keys: Union[str, bool])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, bool]</code> <p>Optional keys for logging. If set to <code>True</code> it will log all keys.</p> <code>()</code>"},{"location":"core/thunder_module/","title":"ThunderModule","text":"<p>ThunderModule inherits everything from LightningModule and implements essential methods for most common training pipelines.</p>"},{"location":"core/thunder_module/#from-lightning-to-thunder","title":"From Lightning to Thunder","text":"<p>Most common pipelines are implemented in lightning in the following way: <pre><code>from lightning import LightningModule\nclass Model(LightningModule):\ndef __init__(self):\nself.architecture: nn.Module = ...\nself.metrics = ... # smth like Dict[str, Callable]\ndef forward(self, *args, **kwargs):\nreturn self.architecture(*args, **kwargs)\ndef criterion(self, x, y):\n...\ndef training_step(self, batch, batch_idx):\nx, y = batch\nreturn self.criterion(self(x), y)\ndef validation_step(self, batch, batch_idx, dataloader_idx):\n# forward and metrics computation or output preservation\n...\ndef test_step(self, batch, batch_idx, dataloader_idx):\n# forward and metrics computation or output preservation\n...\ndef configure_optimizers(self):\nreturn Adam(...), StepLR(...)\n</code></pre></p> <p>ThunderModule offers an implementation of necessary steps shown above. <pre><code>from thunder import ThunderModule\narchitecture: nn.Module = ...\ncriterion = CrossEntropy()\noptimizer = Adam(architecture.parameters())\nscheduler = StepLR(optimizer)\nmodel = ThunderModule(architecture, criterion,\noptimizer=optimizer, lr_scheduler=scheduler)\n</code></pre></p>"},{"location":"core/thunder_module/#configuring-optimizers","title":"Configuring Optimizers","text":"<p>For extra information see this. Lightning requires optimizers and learning rate policies to be defined inside <code>configure_optimizers</code> method. Using ThunderModule allows you to pass the following configurations of  optimizers and learning rate schedulers:</p> <pre><code>from torch import nn\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom torch.optim import Adam\narchitecture = nn.Linear(2, 2)\n</code></pre>"},{"location":"core/thunder_module/#no-scheduling","title":"No scheduling","text":"<pre><code>optimizer = Adam(architecture.parameters())\nmodel = ThunderModule(..., optimizer=optimizer)\n</code></pre>"},{"location":"core/thunder_module/#defining-optimizer-and-scheduler","title":"Defining optimizer and scheduler","text":"<pre><code>optimizer = Adam(architecture.parameters())\nlr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., optimizer=optimizer, lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#defining-no-optimizer","title":"Defining no optimizer","text":"<pre><code>lr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#multiple-optimizers","title":"Multiple Optimizers","text":"<p>Thunder just as lightning supports configuration with more than 1 optimizer. If such configuration is to be used, manual optimization is required. Guide on manual optimization</p> <p>In thunder you can pass lists of optimizers and schedulers to ThunderModule. <pre><code>class ThunderModuleManual(ThunderModule):\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.automatic_optimization = False\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Scheduler(opt) for opt in optimizers]\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p>"},{"location":"core/thunder_module/#thunder-policies","title":"Thunder Policies","text":"<p>As shown above, torch schedulers require optimizer(s) to be passed to them before they are given to ThunderModule. It is not very convenient and also they lack some basic  functionality. You can use thunder policies just like torch schedulers: <pre><code>from thunder.policy import Switch\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Switch({1: 0.001}), Switch({2: 0.001})]\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p> <p>For extra information see Thunder Policies Docs.</p>"},{"location":"core/thunder_module/#inference","title":"Inference","text":"<p>During inference step, ThunderModule uses Predictors in order to preprocess data and make inverse transformsa after passing data through the model. Default predictor is just an identity function.</p> <p>For more on predictors see Thunder Predictors Docs.</p>"},{"location":"core/thunder_module/#batch-transfer","title":"Batch Transfer","text":"<p>ThunderModule transfers training batches to device by default. However, during  inference batch remains on the device, on which it was received from data loader.  Transfering happens later in the <code>inference_step</code>, which is invoked in <code>validation_step</code>, <code>test_step</code> and <code>predict_step</code>.</p>"},{"location":"core/thunder_module/#reference","title":"Reference","text":""},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule","title":"thunder.torch.core.ThunderModule","text":"<pre><code>thunder.torch.core.ThunderModule(architecture: nn.Module, criterion: Callable, n_targets: int = 1, activation: Callable = identity, optimizer: Union[List[Optimizer], Optimizer] = None, lr_scheduler: Union[List[LRScheduler], LRScheduler] = None, predictor: BasePredictor = None, n_val_targets: int = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>nn.Module</code> <p>Model architecture used to conduct forward pass.</p> required <code>criterion</code> <code>Callable</code> <p>Criterion to optimize.</p> required <code>n_targets</code> <code>int</code> <p>Number of target values in train and inference batches, if negative, then ...</p> <code>1</code> <code>activation</code> <code>Callable</code> <p>Final activation function for inference, identity by default.</p> <code>identity</code> <code>optimizer</code> <code>Union[List[Optimizer], Optimizer]</code> <p>Optimizers.</p> <code>None</code> <code>lr_scheduler</code> <code>Union[List[LRScheduler], LRScheduler]</code> <p>Learning Rate policies.</p> <code>None</code> <code>predictor</code> <code>BasePredictor</code> <p>Predictor for inference.</p> <code>None</code> <code>n_val_targets</code> <code>int</code> <p>Number of target values for inference, if set to None assumes value of <code>n_targets</code>.</p> <code>None</code> Source code in <code>thunder/torch/core.py</code> <pre><code>def __init__(\nself,\narchitecture: nn.Module,\ncriterion: Callable,\nn_targets: int = 1,\nactivation: Callable = identity,\noptimizer: Union[List[Optimizer], Optimizer] = None,\nlr_scheduler: Union[List[LRScheduler], LRScheduler] = None,\npredictor: BasePredictor = None,\nn_val_targets: int = None\n):\n\"\"\"\n    Parameters\n    ----------\n    architecture: nn.Module\n        Model architecture used to conduct forward pass.\n    criterion: Callable\n        Criterion to optimize.\n    n_targets: int\n        Number of target values in train and inference batches, if negative, then ...\n    activation: Callable\n        Final activation function for inference, identity by default.\n    optimizer: Union[List[Optimizer], Optimizer]\n        Optimizers.\n    lr_scheduler: Union[List[LRScheduler], LRScheduler]\n        Learning Rate policies.\n    predictor: BasePredictor.\n        Predictor for inference.\n    n_val_targets: int\n        Number of target values for inference, if set to None assumes value of `n_targets`.\n    \"\"\"\nsuper().__init__()\nself.architecture = architecture\nself.criterion = criterion\nself.n_targets = n_targets\nself.n_val_targets = n_targets if n_val_targets is None else n_val_targets\nself.activation = activation\nself.optimizer = optimizer\nself.lr_scheduler = lr_scheduler\nself.predictor = predictor if predictor else Predictor()\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.training_step","title":"training_step","text":"<pre><code>training_step(batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT:\nx, y = batch[: -self.n_targets], batch[-self.n_targets:]\nreturn self.criterion(self(*x), *y)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def validation_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.test_step","title":"test_step","text":"<pre><code>test_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def test_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.predict_step","title":"predict_step","text":"<pre><code>predict_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def predict_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.inference_step","title":"inference_step","text":"<pre><code>inference_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def inference_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nx, y = batch[:-self.n_val_targets], batch[-self.n_val_targets:]\nreturn self.predictor([x], self.predict)[0], y\n</code></pre>"},{"location":"policy/","title":"Policy","text":"<p>Policies are objects that define how some value changes through time. Good example of them is Learning Rate Schedulers.  </p>"},{"location":"policy/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Contrary to default PyTorch Learning Rate schedulers, ours does not require an optimizer to be passed during initialization.</p> <p>See Learning Rate Schedulers docs</p>"},{"location":"policy/lr_schedulers/","title":"LR Schedulers","text":"<p>All schedulers in thunder are subclasses of <code>torch.optim.lr_scheduler.LRScheduler</code>. However, during initialization they do not require optimizer to be passed.</p>"},{"location":"policy/lr_schedulers/#usage","title":"Usage","text":"<p>We will use Switch as an example. <pre><code>from thunder.policy import Switch\nswitch = Switch({10: 0.001, 20: 0.001 / 10})\n</code></pre> We have just created a policy, but to make it work, it still needs an optimizers. Let's see how it works after being assembled. <pre><code>optimizer = Adam(...)\nscheduler(optimizer) # bounds optimizer to scheduler\n# or \n# scheduler = scheduler(optimizer)\n# You can also retrieve optimizer:\nopt = scheduler.optimizer\n</code></pre> After assigning optimizer to scheduler, policy instance will work just like usual torch scheduler.</p>"},{"location":"policy/lr_schedulers/#initial-lr","title":"Initial LR","text":"<p>All schedulers have <code>lr_init</code> parameters, if specified, it will be used as lr value on 0th step.</p>"},{"location":"policy/lr_schedulers/#reference","title":"Reference","text":""},{"location":"policy/lr_schedulers/#thunder.policy","title":"thunder.policy","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Multiply","title":"Multiply","text":"<p>Multiplies learning rate value on the specified factor in <code>mapping</code>. Example:     <pre><code>    sch = Multiply({1: 0.1, 4: 0.3})\n</code></pre>     if initial learning rate is 1e-3, learning rate will be: 1e-3, 1e-4, 1e-4, 1e-4, 3-e5, ...</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Maps epoch to factor, keeping the last value between the epochs.</p> required <code>lr_init</code> <p>Initial learning rate for each group of parameters.</p> required"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping: Union[List[Dict[int, float]], Dict[int, float]]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule","title":"Schedule","text":"<p>Assigns learning rate values received from callable mapping. Example:     <pre><code>sch = Schedule(np.cos)\n</code></pre>     lr will have values of np.cos(epoch_number)</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Maps epoch to value.</p> required <code>lr_init</code> <p>Initial learning rate for each group of parameters.</p> required"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping: Union[List[Callable], Callable]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch","title":"Switch","text":"<p>Assigns learning rate values received from dict mapping. Example:     <pre><code>sch = Switch({0: 1e-4, 2: 1e-10)\n</code></pre>     lr: 1e-4, 1e-4, 1e-10, 1e-10, ...</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Maps specified epochs to specified values, preserving learning rate between epochs.</p> required <code>lr_init</code> <p>Initial learning rate for each group of parameters.</p> required"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping: Union[List[Dict[int, float]], Dict[int, float]]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre>"},{"location":"policy/lr_schedulers/#base-classes","title":"Base classes","text":""},{"location":"policy/lr_schedulers/#thunder.policy","title":"thunder.policy","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Policy","title":"Policy","text":"<pre><code>Policy()\n</code></pre> <p>Policy base class.</p> Source code in <code>thunder/policy.py</code> <pre><code>def __init__(self):\npass\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.__call__","title":"__call__","text":"<pre><code>__call__(optimizer: Optimizer) -&gt; Policy\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def __call__(self, optimizer: Optimizer) -&gt; Policy:\nself.set_optimizer(optimizer)\nreturn self\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.set_optimizer","title":"set_optimizer","text":"<pre><code>set_optimizer(optimizer: Optimizer) -&gt; None\n</code></pre> <p>Assigns optimizer to a scheduler</p> Source code in <code>thunder/policy.py</code> <pre><code>def set_optimizer(self, optimizer: Optimizer) -&gt; None:\n\"\"\"Assigns optimizer to a scheduler\"\"\"\nsuper().__init__(optimizer)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.get_lr","title":"get_lr  <code>abstractmethod</code>","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre> <p>Computes new value of learning rate.</p> <p>Returns:</p> Type Description <code>List[float]</code> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef get_lr(self) -&gt; List[float]:\n\"\"\"\n    Computes new value of learning rate.\n    Returns\n    -------\n    List[float]\n    \"\"\"\npass\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.state_dict","title":"state_dict  <code>abstractmethod</code>","text":"<pre><code>state_dict(*keys: str) -&gt; Dict[str, Any]\n</code></pre> <p>Creates state dict of scheduler, excluding optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str</code> <p>Names of attributes to be excluded from state_dict</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef state_dict(self, *keys: str) -&gt; Dict[str, Any]:\n\"\"\"\n    Creates state dict of scheduler, excluding optimizer.\n    Parameters\n    ----------\n    keys: str\n        Names of attributes to be excluded from state_dict\n    Returns\n    -------\n    Dict[str, Any]\n    \"\"\"\nkeys = (*keys, \"optimizer\")\nreturn {key: value for key, value in self.__dict__.items() if key not in keys}\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.load_state_dict","title":"load_state_dict  <code>abstractmethod</code>","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Loads state dict of scheduler</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>State dict of scheduler.</p> required Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n\"\"\"\n    Loads state dict of scheduler\n    Parameters\n    ----------\n    state_dict: Dict[str, Any]\n        State dict of scheduler.\n    \"\"\"\nself.__dict__.update(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy","title":"MappingPolicy","text":"<pre><code>MappingPolicy(mapping, lr_init: Union[List[float], float] = 0.001)\n</code></pre> <p>Base class for policy with mapping. Mapping can be a dict or a function (it should also be a list of latter types in case of multiple param groups). Mapping is the binding between epoch or step number and learning rate value.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Binding of epoch or step number and learning rate.</p> required <code>lr_init</code> <code>Union[List[float], float]</code> <p>Initial learning rate for each group of parameters.</p> <code>0.001</code> Source code in <code>thunder/policy.py</code> <pre><code>def __init__(self, mapping, lr_init: Union[List[float], float] = 1e-3):\n\"\"\"\n    Base class for policy with mapping. Mapping can be a dict or a function\n    (it should also be a list of latter types in case of multiple param groups).\n    Mapping is the binding between epoch or step number and learning rate value.\n    Parameters\n    ----------\n    mapping\n        Binding of epoch or step number and learning rate.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\nself.current_mapping = None\nself.mapping = mapping\nself.current_lr_init = None\nself.lr_init = lr_init\nsuper().__init__()\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.current_mapping","title":"current_mapping  <code>instance-attribute</code>","text":"<pre><code>current_mapping = None\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping = mapping\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.current_lr_init","title":"current_lr_init  <code>instance-attribute</code>","text":"<pre><code>current_lr_init = None\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.lr_init","title":"lr_init  <code>instance-attribute</code>","text":"<pre><code>lr_init = lr_init\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.set_optimizer","title":"set_optimizer","text":"<pre><code>set_optimizer(optimizer: Optimizer) -&gt; None\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def set_optimizer(self, optimizer: Optimizer) -&gt; None:\nself.current_mapping = self.mapping\nif isinstance(self.mapping, dict) or callable(self.mapping):\nself.current_mapping = [deepcopy(self.mapping) for _ in optimizer.param_groups]\nself.current_lr_init = self.lr_init\nif isinstance(self.lr_init, (float, int)):\nself.current_lr_init = [self.lr_init for _ in optimizer.param_groups]\nif len(self.current_mapping) != len(optimizer.param_groups):\nraise ValueError(f\"Got {len(self.current_mapping)} mappings and {len(optimizer.param_groups)} param groups\")\nif len(self.current_lr_init) != len(optimizer.param_groups):\nraise ValueError(f\"Got {len(self.current_lr_init)} lr_init and {len(optimizer.param_groups)} param groups\")\nfor lr_init, param_group in zip(self.current_lr_init, optimizer.param_groups):\nparam_group[\"lr\"] = lr_init\nsuper().set_optimizer(optimizer)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def __repr__(self) -&gt; str:\nmapping = self.current_mapping if self.current_mapping else self.mapping\nlr_init = self.current_lr_init if self.current_lr_init is not None else self.lr_init\nreturn f\"{self.__class__.__name__}({mapping=}, {lr_init=})\"\n</code></pre>"}]}