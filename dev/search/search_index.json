{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>You saw the lightning. Now it's time to hear the thunder</p>"},{"location":"#thunder","title":"Thunder","text":"<p>The Deep Learning framework based on Lighnting</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install from pypi <pre><code>pip install thunder\n</code></pre> or directly from GitHub <pre><code>git clone https://github.com/neuro-ml/thunder.git\ncd thunder &amp;&amp; pip install -e .\n</code></pre></p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#thundermodule","title":"ThunderModule","text":""},{"location":"#metriclogger","title":"MetricLogger","text":""},{"location":"#experiment-configs","title":"Experiment configs","text":""},{"location":"#cli-integrations-with-wandb","title":"CLI &amp; Integrations with WandB","text":""},{"location":"callbacks/","title":"Callbacks","text":"<p>Lightning Callbacks allow you to modify your training pipelines. For extra information see this.</p>"},{"location":"callbacks/#thunder-callbacks","title":"Thunder Callbacks","text":"Name Description MetricLogger Computes metrics and logs them TimeProfiler Logs the time of each LightningModule's step"},{"location":"callbacks/metric_logger/","title":"MetricLogger","text":"<p>This callback takes on computation and aggregation of the specified metrics.  </p>"},{"location":"callbacks/metric_logger/#usage","title":"Usage","text":""},{"location":"callbacks/metric_logger/#group-metrics","title":"Group metrics","text":"<p>Group metrics are computed on the entire dataset. For example, you want to compute classification accuracy on MNIST. <pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(group_metrics={\"accuracy\": accuracy_score})])\n</code></pre></p> <p>If you use any loggers (e.g. <code>Tensorboard</code> or <code>WandB</code>), <code>accuracy</code> will appear in them as follows: <code>val/accuracy</code> - validation metrics. <code>test/accuracy</code> - test metrics.</p>"},{"location":"callbacks/metric_logger/#single-metrics","title":"Single metrics","text":"<p>Single metrics are computed on each object separately and only then aggregated. It is a common use case for tasks like segmentation or object detection.</p>"},{"location":"callbacks/metric_logger/#simple-use-case","title":"Simple use case","text":"<p><pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(single_metrics={\"accuracy\": accuracy_score})])\n</code></pre> MetricLogger will log mean values by default. But you can add custom aggregations as well.</p>"},{"location":"callbacks/metric_logger/#custom-aggregations","title":"Custom aggregations","text":"<p>Let see what can be done if we want to log <code>std</code> of metrics as well as mean values. <pre><code>import numpy as np\nfrom thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\naggregate_fn = np.std\nmetric_logger = MetricLogger(single_metrics={\"accuracy\": accuracy_score},\naggregate_fn=aggregate_fn) \ntrainer = Trainer(callbacks=[metric_logger])\n</code></pre> The mean values appera in loggers with no additional keys.  MetricCallback will try to infer the name of an aggregating function and use it as an additional key.</p> <p><code>val/accuracy</code> - validation mean accuracy. <code>val/std/accuracy</code> - validation accuracy std. <code>test/accuracy</code> - test mean accuracy. <code>test/std/accuracy</code> - test accuracy std.</p> <p><code>aggregate_fn</code> can also be specified as follows:</p> <p><pre><code>import numpy as np\naggregate_fn = [np.std, np.median]\naggregate_fn = [np.std, \"median\", \"max\", \"min\"]\naggregate_fn = {\"zero\": lambda x: x[0]}\n</code></pre> MetricLogger can accept <code>str</code> or <code>List[str]</code> as <code>aggregate_fn</code>,  in this format it supports the following metrics:</p> Name Function \"median\" <code>np.median</code> \"min\" <code>np.min</code> \"max\" <code>np.max</code> \"std\" <code>np.std</code>"},{"location":"callbacks/metric_logger/#preprocessing","title":"Preprocessing","text":"<p>Sometimes metrics require some preprocessing. In this case, keys of <code>single_metrics</code> dict must be callable objects. <pre><code>from sklearn.metrics import accuracy_score, recall_score\nthreshold = lambda x, y: (x &gt; 0.5, y)\nsingle_metrics = {threshold: [accuracy_score, recall_score()]} \n# or\nsingle_metrics = {threshold: {\"acc\": accuracy_score, \"rec\": recall_score}}\n# or\nsingle_metrics = {threshold: recall_score}\n...\n</code></pre></p>"},{"location":"callbacks/metric_logger/#reference","title":"Reference","text":""},{"location":"callbacks/metric_logger/#thunder.callbacks.metric_logger.MetricLogger","title":"thunder.callbacks.metric_logger.MetricLogger","text":"<pre><code>thunder.callbacks.metric_logger.MetricLogger(single_metrics: Dict = None, group_metrics: Dict[str, Callable] = None, aggregate_fn: Union[Dict[str, Callable], str, Callable, List[Union[str, Callable]]] = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>single_metrics</code> <code>Dict</code> <p>Metrics that are calculated on each object separately and then aggregated.</p> <code>None</code> <code>group_metrics</code> <code>Dict[str, Callable]</code> <p>Metrics that are calculated on entire dataset.</p> <code>None</code> <code>aggregate_fn</code> <code>Union[Dict[str, Callable], str, Callable, List[Union[str, Callable]]]</code> <p>How to aggregate metrics. By default computes mean value. If yoy specify somethind, then the callback will compute mean and the specified values.</p> <code>None</code>"},{"location":"callbacks/time_profiler/","title":"Time profiler","text":""},{"location":"callbacks/time_profiler/#timeprofiler","title":"TimeProfiler","text":"<p>Lightning Callback which allows you to measure the time each step takes and log it during the training process.</p>"},{"location":"callbacks/time_profiler/#logged-values","title":"Logged values","text":"<p>TimeProfiler logs the following steps:</p> Name Logged by default Description train batch Time taken by forward, optimizer step, and backward during train step. validation batch Time taken by forward during validation step. train epoch Time taken by train epoch without validation. validation epoch Time taken by validation epoch. avg train downtime* Average downtime in training step. avg val downtime Average downtime in validation step. backward Time taken by backprop. optimizer step Time taken by optimizer. total train downtime Total downtime in training epoch. total val downtime Total downtime in validation epoch. <p>*Downtime - the process during which model does not work (e.g. data loader is working now)  </p>"},{"location":"callbacks/time_profiler/#usage","title":"Usage","text":"<pre><code>from thunder.callbacks import TimeProfiler\nfrom lightning import Trainer\n# logs default keys and in addition backward and optimizer step\ntrainer = Trainer(callbacks=[TimeProfiler(\"backward\", \"optimizer step\")])\n</code></pre>"},{"location":"callbacks/time_profiler/#reference","title":"Reference","text":""},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler","title":"thunder.callbacks.time_profiler.TimeProfiler","text":"<pre><code>thunder.callbacks.time_profiler.TimeProfiler(*keys: Union[str, bool])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, bool]</code> <p>Optional keys for logging. If set to <code>True</code> it will log all keys.</p> <code>()</code>"},{"location":"cli/","title":"Command Line Interface","text":"<p>Thunder provides its users with CLI to bring convenience and comfort into experiment building and execution routine.</p>"},{"location":"cli/#requirements","title":"Requirements:","text":"<p>lazycon</p>"},{"location":"cli/#building-an-experiment","title":"Building an experiment","text":"<p>In order to build an experiment, you can execute the follwing command: <pre><code>thunder build /path/to/config /path/to/experiment\n</code></pre> It will create a folder with built configs in it.  </p>"},{"location":"cli/#overriding-config-entries","title":"Overriding config entries","text":"<p>While conducting experiments one can find themselves in constant need of changing significant number of parameters. But it is not convenient to always do it via IDE or any other code editor.  Thunder gives an ability to override the values while building an experiment.</p> <p>If in your config you have <pre><code>batch_size = 1\nlr = 0.01\n</code></pre> You can override it using <code>-u</code> flag: <pre><code>thunder build /path/to/config /path/to/experiment -u batch_size 2 -u lr 0.001 </code></pre> <code>batch_size</code> and <code>lr</code> will be assigned 2 and 0.001 respectively.</p>"},{"location":"cli/#running-an-experiment","title":"Running an experiment","text":"<p>You can run built experiment by executing the next command: <pre><code>thunder run /path/to/experiment\n</code></pre> Under the hood thunder extracts necessary entries (e.g. model and trainer) from your built config and executes <code>trainer.run(model, train_data, ...)</code>.</p> <p>You can also specify run parameters, like amount of RAM and number of CPUs.  ...</p>"},{"location":"cli/#predefined-run-configs","title":"Predefined run configs","text":""},{"location":"cli/#wandb-sweeps-integration","title":"WandB Sweeps integration","text":"<p>WandB has hyperparameters tuning system called Sweeps. Sweeps allow you to run multiple experiment with predefined grid of parameters and compare run results. However, we find default sweep execution system very inconvenient when it comes to running experiments on cluster.</p> <p>After running a few experiments with WandB Logger, you can create sweep configuration. WandB will give a command <code>wandb agent project/sweep_id</code>. You can copy it and paste it into the following command: <pre><code>thunder PASTE_HERE /path/to/config /path/to/experiment </code></pre></p>"},{"location":"configs/","title":"Lazycon","text":"<p>Thunder embraces the power of lazycon allowing you to build configs for your  experiments. </p>"},{"location":"configs/#config-structure","title":"Config structure","text":"<p>In order for the config to be correct it should contain the following objects: <code>trainer</code> - Lightning Trainer <code>model</code> - instance of LightningModule or ThunderModule <code>train_data</code> - train dataloader <code>val_data</code> - val dataloader (Optional) <code>test_data</code> - test dataloader (Optional)</p>"},{"location":"configs/#executing-a-config","title":"Executing a config","text":"<p>Configs can be run just like usual python files: <pre><code>python /path/to/config.config\n</code></pre></p> <p>But Thunder has its own command line interface,  about which you can read here.</p>"},{"location":"core/thunder_module/","title":"ThunderModule","text":"<p>ThunderModule inherits everything from LightningModule and implements essential methods for most common training pipelines.</p>"},{"location":"core/thunder_module/#from-lightning-to-thunder","title":"From Lightning to Thunder","text":"<p>Most common pipelines are implemented in lightning in the following way: <pre><code>from lightning import LightningModule\nclass Model(LightningModule):\ndef __init__(self):\nself.architecture: nn.Module = ...\nself.metrics = ... # smth like Dict[str, Callable]\ndef forward(self, *args, **kwargs):\nreturn self.architecture(*args, **kwargs)\ndef criterion(self, x, y):\n...\ndef training_step(self, batch, batch_idx):\nx, y = batch\nreturn self.criterion(self(x), y)\ndef validation_step(self, batch, batch_idx, dataloader_idx):\n# forward and metrics computation or output preservation\n...\ndef test_step(self, batch, batch_idx, dataloader_idx):\n# forward and metrics computation or output preservation\n...\ndef configure_optimizers(self):\nreturn Adam(...), StepLR(...)\n</code></pre></p> <p>ThunderModule offers an implementation of necessary steps shown above. <pre><code>from thunder import ThunderModule\narchitecture: nn.Module = ...\ncriterion = CrossEntropy()\noptimizer = Adam(architecture.parameters())\nscheduler = StepLR(optimizer)\nmodel = ThunderModule(architecture, criterion,\noptimizer=optimizer, lr_scheduler=scheduler)\n</code></pre></p>"},{"location":"core/thunder_module/#configuring-optimizers","title":"Configuring Optimizers","text":"<p>For extra information see this. Lightning requires optimizers and learning rate policies to be defined inside <code>configure_optimizers</code> method. Using ThunderModule allows you to pass the following configurations of  optimizers and learning rate schedulers:</p> <pre><code>from torch import nn\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom torch.optim import Adam\narchitecture = nn.Linear(2, 2)\n</code></pre>"},{"location":"core/thunder_module/#no-scheduling","title":"No scheduling","text":"<pre><code>optimizer = Adam(architecture.parameters())\nmodel = ThunderModule(..., optimizer=optimizer)\n</code></pre>"},{"location":"core/thunder_module/#defining-optimizer-and-scheduler","title":"Defining optimizer and scheduler","text":"<pre><code>optimizer = Adam(architecture.parameters())\nlr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., optimizer=optimizer, lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#defining-no-optimizer","title":"Defining no optimizer","text":"<pre><code>lr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#multiple-optimizers","title":"Multiple Optimizers","text":"<p>Thunder just as lightning supports configuration with more than 1 optimizer. If such configuration is to be used, manual optimization is required. Guide on manual optimization</p> <p>In thunder you can pass lists of optimizers and schedulers to ThunderModule. <pre><code>class ThunderModuleManual(ThunderModule):\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.automatic_optimization = False\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Scheduler(opt) for opt in optimizers]\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p>"},{"location":"core/thunder_module/#thunder-policies","title":"Thunder Policies","text":"<p>As shown above, torch schedulers require optimizer(s) to be passed to them before they are given to ThunderModule. It is not very convenient and also they lack some basic  functionality. You can use thunder policies just like torch schedulers: <pre><code>from thunder.policy import Switch\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Switch({1: 0.001}), Switch({2: 0.001})]\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p> <p>For extra information see Thunder Policies Docs.</p>"},{"location":"core/thunder_module/#inference","title":"Inference","text":"<p>During inference step, ThunderModule uses Predictors in order to preprocess data and make inverse transformsa after passing data through the model. Default predictor is just an identity function.</p> <p>For more on predictors see Thunder Predictors Docs.</p>"},{"location":"core/thunder_module/#batch-transfer","title":"Batch Transfer","text":"<p>ThunderModule transfers training batches to device by default. However, during  inference batch remains on the device, on which it was received from data loader.  Transfering happens later in the <code>inference_step</code>, which is invoked in <code>validation_step</code>, <code>test_step</code> and <code>predict_step</code>.</p>"},{"location":"core/thunder_module/#reference","title":"Reference","text":""},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule","title":"thunder.torch.core.ThunderModule","text":"<pre><code>thunder.torch.core.ThunderModule(architecture: nn.Module, criterion: Callable, n_targets: int = 1, activation: Callable = identity, optimizer: Union[List[Optimizer], Optimizer] = None, lr_scheduler: Union[List[LRScheduler], LRScheduler] = None, predictor: BasePredictor = None, n_val_targets: int = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>nn.Module</code> <p>Model architecture used to conduct forward pass.</p> required <code>criterion</code> <code>Callable</code> <p>Criterion to optimize.</p> required <code>n_targets</code> <code>int</code> <p>Number of target values in train and inference batches, if negative, then ...</p> <code>1</code> <code>activation</code> <code>Callable</code> <p>Final activation function for inference, identity by default.</p> <code>identity</code> <code>optimizer</code> <code>Union[List[Optimizer], Optimizer]</code> <p>Optimizers.</p> <code>None</code> <code>lr_scheduler</code> <code>Union[List[LRScheduler], LRScheduler]</code> <p>Learning Rate policies.</p> <code>None</code> <code>predictor</code> <code>BasePredictor</code> <p>Predictor for inference.</p> <code>None</code> <code>n_val_targets</code> <code>int</code> <p>Number of target values for inference, if set to None assumes value of <code>n_targets</code>.</p> <code>None</code> Source code in <code>thunder/torch/core.py</code> <pre><code>def __init__(\nself,\narchitecture: nn.Module,\ncriterion: Callable,\nn_targets: int = 1,\nactivation: Callable = identity,\noptimizer: Union[List[Optimizer], Optimizer] = None,\nlr_scheduler: Union[List[LRScheduler], LRScheduler] = None,\npredictor: BasePredictor = None,\nn_val_targets: int = None\n):\n\"\"\"\n    Parameters\n    ----------\n    architecture: nn.Module\n        Model architecture used to conduct forward pass.\n    criterion: Callable\n        Criterion to optimize.\n    n_targets: int\n        Number of target values in train and inference batches, if negative, then ...\n    activation: Callable\n        Final activation function for inference, identity by default.\n    optimizer: Union[List[Optimizer], Optimizer]\n        Optimizers.\n    lr_scheduler: Union[List[LRScheduler], LRScheduler]\n        Learning Rate policies.\n    predictor: BasePredictor.\n        Predictor for inference.\n    n_val_targets: int\n        Number of target values for inference, if set to None assumes value of `n_targets`.\n    \"\"\"\nsuper().__init__()\nself.architecture = architecture\nself.criterion = criterion\nself.n_targets = n_targets\nself.n_val_targets = n_targets if n_val_targets is None else n_val_targets\nself.activation = activation\nself.optimizer = optimizer\nself.lr_scheduler = lr_scheduler\nself.predictor = predictor if predictor else Predictor()\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.training_step","title":"training_step","text":"<pre><code>training_step(batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT:\nx, y = batch[: -self.n_targets], batch[-self.n_targets:]\nreturn self.criterion(self(*x), *y)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def validation_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.test_step","title":"test_step","text":"<pre><code>test_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def test_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.predict_step","title":"predict_step","text":"<pre><code>predict_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def predict_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.inference_step","title":"inference_step","text":"<pre><code>inference_step(batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any\n</code></pre> Source code in <code>thunder/torch/core.py</code> <pre><code>def inference_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nx, y = batch[:-self.n_val_targets], batch[-self.n_val_targets:]\nreturn self.predictor([x], self.predict)[0], y\n</code></pre>"},{"location":"layout/","title":"Layout","text":"<p>Layout instances are responsible for splitting your datasets managing which data fold is used for each experiment. They also check reproducibility of your data splits.</p>"},{"location":"layout/#thunder-layouts","title":"Thunder Layouts","text":"Name Description Split Layout for K fold cross-validation SingleSplit Layout with several sets (e.g. train + val + test) <p>All Layout subclasses follow common interface </p>"},{"location":"layout/#thunder.layout.interface.Layout","title":"thunder.layout.interface.Layout","text":""},{"location":"layout/#thunder.layout.interface.Layout.build","title":"build  <code>abstractmethod</code>","text":"<pre><code>build(experiment: Path, config: Config) -&gt; Iterable[Node]\n</code></pre> Source code in <code>thunder/layout/interface.py</code> <pre><code>@abstractmethod\ndef build(self, experiment: Path, config: Config) -&gt; Iterable[Node]:\npass\n</code></pre>"},{"location":"layout/#thunder.layout.interface.Layout.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]\n</code></pre> Source code in <code>thunder/layout/interface.py</code> <pre><code>@abstractmethod\ndef load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\npass\n</code></pre>"},{"location":"layout/#thunder.layout.interface.Layout.set","title":"set  <code>abstractmethod</code>","text":"<pre><code>set(**kwargs)\n</code></pre> Source code in <code>thunder/layout/interface.py</code> <pre><code>@abstractmethod\ndef set(self, **kwargs):\npass\n</code></pre>"},{"location":"layout/splits/","title":"Splits","text":""},{"location":"layout/splits/#thunder.layout.split.Split","title":"thunder.layout.split.Split","text":"<pre><code>thunder.layout.split.Split(split: Callable, entries: Sequence, *args: Any, names: Optional[Sequence[str]] = None, **kwargs: Any)\n</code></pre> <p>Splits data according to split function.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>Callable</code> <p>Split function.</p> required <code>entries</code> <code>Sequence</code> <p>Series of ids or torch Dataset or Connectome Layer.</p> required <code>args</code> <code>Any</code> <p>args for split.</p> <code>()</code> <code>names</code> <code>Optional[Sequence[str]]</code> <p>Names of folds, e.g. 'train', 'val', test'</p> <code>None</code> <code>kwargs</code> <code>Any</code> <code>{}</code> <code>kwargs</code> <code>Any</code> <code>{}</code> Source code in <code>thunder/layout/split.py</code> <pre><code>def __init__(self, split: Callable, entries: Sequence, *args: Any, names: Optional[Sequence[str]] = None,\n**kwargs: Any):\n\"\"\"\n    Splits data according to split function.\n    Parameters\n    ----------\n    split: Callable\n        Split function.\n    entries: Sequence\n        Series of ids or torch Dataset or Connectome Layer.\n    args: Any\n        args for split.\n    names: Optional[Sequence[str]]\n        Names of folds, e.g. 'train', 'val', test'\n    kwargs: Any\n    kwargs for split.\n    \"\"\"\nif not callable(split):\nif not hasattr(split, 'split'):\n# TODO\nraise TypeError(split)\nsplit = split.split\nids = entries_to_ids(entries)\n# TODO: safer way to unify types\nsplits = [tuple(map(jsonify, xs)) for xs in split(ids, *args, **kwargs)]\nif names is not None:\n# TODO\nassert len(set(names)) == len(names)\nassert len(splits[0]) == len(names)\nself.entries = entries\nself.splits = splits\nself.names = names\nself.fold: Optional[int] = None\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.build","title":"build","text":"<pre><code>build(experiment: Path, config: Config)\n</code></pre> Source code in <code>thunder/layout/split.py</code> <pre><code>def build(self, experiment: Path, config: Config):\nconfig.dump(experiment / 'experiment.config')\nname = experiment.name\nfor fold, split in enumerate(self.splits):\nfolder = experiment / f'fold_{fold}'\nfolder.mkdir()\nsave(split, folder / 'split.json')\nlocal = config.copy().update(ExpName=f'{name}({fold})', GroupName=name)\nlocal.dump(folder / 'experiment.config')\nyield Node(name=str(fold))\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.load","title":"load","text":"<pre><code>load(experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]\n</code></pre> Source code in <code>thunder/layout/split.py</code> <pre><code>def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\nfolder = experiment / f'fold_{node.name}'\nreturn Config.load(folder / 'experiment.config'), folder, {\n'fold': int(node.name),\n'split': tuple(load(folder / 'split.json')),\n}\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.set","title":"set","text":"<pre><code>set(fold: int, split: Optional[Sequence[Sequence]] = None)\n</code></pre> Source code in <code>thunder/layout/split.py</code> <pre><code>def set(self, fold: int, split: Optional[Sequence[Sequence]] = None):\nself.fold = fold\nif split is None:\nwarnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\nelse:\nif split != self.splits[fold]:\n# TODO: consistency error?\nraise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit","title":"thunder.layout.split.SingleSplit","text":"<pre><code>thunder.layout.split.SingleSplit(entries: Sequence, *, shuffle: bool = True, random_state: Union[np.random.RandomState, int, None] = 0, **sizes: Union[int, float])\n</code></pre> <p>Creates single fold experiment, with custom number of sets.</p> <p>Parameters:</p> Name Type Description Default <code>entries</code> <code>Sequence</code> <p>Sequence of ids or</p> required <code>shuffle</code> <code>bool</code> <p>Whether to shuffle entries.</p> <code>True</code> <code>random_state</code> <code>Union[np.random.RandomState, int, None]</code> <code>0</code> <code>sizes</code> <code>Union[int, float]</code> <code>{}</code> Source code in <code>thunder/layout/split.py</code> <pre><code>def __init__(self, entries: Sequence, *, shuffle: bool = True,\nrandom_state: Union[np.random.RandomState, int, None] = 0,\n**sizes: Union[int, float]):\n\"\"\"\n    Creates single fold experiment, with custom number of sets.\n    Parameters\n    ----------\n    entries: Sequence\n        Sequence of ids or\n    shuffle: bool\n        Whether to shuffle entries.\n    random_state : Union[np.random.RandomState, int, None]\n    sizes: Union[int, float]\n    \"\"\"\nif not isinstance(random_state, np.random.RandomState):\nrandom_state = np.random.RandomState(random_state)\nids = entries_to_ids(entries)\nself.entries = entries\nself.split = dict(zip(sizes.keys(), multi_split(\nids, list(sizes.values()), shuffle=shuffle, random_state=random_state\n)))\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.build","title":"build","text":"<pre><code>build(experiment: Path, config: Config)\n</code></pre> Source code in <code>thunder/layout/split.py</code> <pre><code>def build(self, experiment: Path, config: Config):\nconfig.dump(experiment / 'experiment.config')\nname = experiment.name\nsave(self.split, experiment / 'split.json')\nlocal = config.copy().update(ExpName=name, GroupName=name)\nlocal.dump(experiment / 'experiment.config')\nreturn []\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.load","title":"load","text":"<pre><code>load(experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]\n</code></pre> Source code in <code>thunder/layout/split.py</code> <pre><code>def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\nreturn Config.load(experiment / 'experiment.config'), experiment, {\n'split': load(experiment / 'split.json'),\n}\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.set","title":"set","text":"<pre><code>set(split: Optional[Dict[str, Sequence]] = None)\n</code></pre> Source code in <code>thunder/layout/split.py</code> <pre><code>def set(self, split: Optional[Dict[str, Sequence]] = None):\nif split is None:\nwarnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\nelse:\nif split != self.split:\n# TODO: consistency error?\nraise ValueError\n</code></pre>"},{"location":"policy/","title":"Policy","text":"<p>Policies are objects that define how some value changes through time. Good example of them is Learning Rate Schedulers.  </p>"},{"location":"policy/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Contrary to default PyTorch Learning Rate schedulers, ours does not require an optimizer to be passed during initialization.</p>"},{"location":"policy/#thunder-schedulers","title":"Thunder Schedulers","text":"Name Description Multiply Multiplies lr on each step by specified factor. Schedule Assigns lr values according to specified callable. Switch Assigns lr values according to specified dict schedule. <p>See Learning Rate Schedulers docs </p>"},{"location":"policy/lr_schedulers/","title":"LR Schedulers","text":"<p>All schedulers in thunder are subclasses of <code>torch.optim.lr_scheduler.LRScheduler</code>. However, during initialization they do not require optimizer to be passed.</p>"},{"location":"policy/lr_schedulers/#usage","title":"Usage","text":"<p>We will use Switch as an example. <pre><code>from thunder.policy import Switch\nswitch = Switch({10: 0.001, 20: 0.001 / 10})\n</code></pre> We have just created a policy, but to make it work, it still needs an optimizers. Let's see how it works after being assembled. <pre><code>optimizer = Adam(...)\nscheduler(optimizer) # bounds optimizer to scheduler\n# or \n# scheduler = scheduler(optimizer)\n# You can also retrieve optimizer:\nopt = scheduler.optimizer\n</code></pre> After assigning optimizer to scheduler, policy instance will work just like usual torch scheduler.</p>"},{"location":"policy/lr_schedulers/#initial-lr","title":"Initial LR","text":"<p>All schedulers have <code>lr_init</code> parameters, if specified, it will be used as lr value on 0th step.</p>"},{"location":"policy/lr_schedulers/#reference","title":"Reference","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Multiply","title":"thunder.policy.Multiply","text":"<p>Multiplies learning rate value on the specified factor in <code>mapping</code>. Example:     <pre><code>    sch = Multiply({1: 0.1, 4: 0.3})\n</code></pre>     if initial learning rate is 1e-3, learning rate will be: 1e-3, 1e-4, 1e-4, 1e-4, 3-e5, ...</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Maps epoch to factor, keeping the last value between the epochs.</p> required <code>lr_init</code> <p>Initial learning rate for each group of parameters.</p> required"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping: Union[List[Dict[int, float]], Dict[int, float]]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def get_lr(self) -&gt; List[float]:\nreturn [\nparam_group[\"lr\"] * mapping.get(self.last_epoch, 1)\nfor param_group, mapping in zip_equal(self.optimizer.param_groups, self.current_mapping)\n]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\nreturn super().state_dict()\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def load_state_dict(self, state_dict):\nsuper().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule","title":"thunder.policy.Schedule","text":"<p>Assigns learning rate values received from callable mapping. Example:     <pre><code>sch = Schedule(np.cos)\n</code></pre>     lr will have values of np.cos(epoch_number)</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Maps epoch to value.</p> required <code>lr_init</code> <p>Initial learning rate for each group of parameters.</p> required"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping: Union[List[Callable], Callable]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def get_lr(self) -&gt; List[float]:\nreturn juxt(self.current_mapping)(self.last_epoch)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\nreturn super().state_dict(\"mapping\")\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\nsuper().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch","title":"thunder.policy.Switch","text":"<p>Assigns learning rate values received from dict mapping. Example:     <pre><code>sch = Switch({0: 1e-4, 2: 1e-10)\n</code></pre>     lr: 1e-4, 1e-4, 1e-10, 1e-10, ...</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Maps specified epochs to specified values, preserving learning rate between epochs.</p> required <code>lr_init</code> <p>Initial learning rate for each group of parameters.</p> required"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping: Union[List[Dict[int, float]], Dict[int, float]]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def get_lr(self) -&gt; List[float]:\nreturn [\nmapping.get(self.last_epoch, param_group[\"lr\"])\nfor param_group, mapping in zip_equal(self.optimizer.param_groups, self.current_mapping)\n]\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\nreturn super().state_dict()\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\nsuper().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#base-classes","title":"Base classes","text":""},{"location":"policy/lr_schedulers/#thunder.policy","title":"thunder.policy","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Policy","title":"Policy","text":"<pre><code>Policy()\n</code></pre> <p>Policy base class.</p> Source code in <code>thunder/policy.py</code> <pre><code>def __init__(self):\npass\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.__call__","title":"__call__","text":"<pre><code>__call__(optimizer: Optimizer) -&gt; Policy\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def __call__(self, optimizer: Optimizer) -&gt; Policy:\nself.set_optimizer(optimizer)\nreturn self\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.set_optimizer","title":"set_optimizer","text":"<pre><code>set_optimizer(optimizer: Optimizer) -&gt; None\n</code></pre> <p>Assigns optimizer to a scheduler</p> Source code in <code>thunder/policy.py</code> <pre><code>def set_optimizer(self, optimizer: Optimizer) -&gt; None:\n\"\"\"Assigns optimizer to a scheduler\"\"\"\nsuper().__init__(optimizer)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.get_lr","title":"get_lr  <code>abstractmethod</code>","text":"<pre><code>get_lr() -&gt; List[float]\n</code></pre> <p>Computes new value of learning rate.</p> <p>Returns:</p> Type Description <code>List[float]</code> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef get_lr(self) -&gt; List[float]:\n\"\"\"\n    Computes new value of learning rate.\n    Returns\n    -------\n    List[float]\n    \"\"\"\npass\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.state_dict","title":"state_dict  <code>abstractmethod</code>","text":"<pre><code>state_dict(*keys: str) -&gt; Dict[str, Any]\n</code></pre> <p>Creates state dict of scheduler, excluding optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str</code> <p>Names of attributes to be excluded from state_dict</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef state_dict(self, *keys: str) -&gt; Dict[str, Any]:\n\"\"\"\n    Creates state dict of scheduler, excluding optimizer.\n    Parameters\n    ----------\n    keys: str\n        Names of attributes to be excluded from state_dict\n    Returns\n    -------\n    Dict[str, Any]\n    \"\"\"\nkeys = (*keys, \"optimizer\")\nreturn {key: value for key, value in self.__dict__.items() if key not in keys}\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.load_state_dict","title":"load_state_dict  <code>abstractmethod</code>","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Loads state dict of scheduler</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>State dict of scheduler.</p> required Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n\"\"\"\n    Loads state dict of scheduler\n    Parameters\n    ----------\n    state_dict: Dict[str, Any]\n        State dict of scheduler.\n    \"\"\"\nself.__dict__.update(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy","title":"MappingPolicy","text":"<pre><code>MappingPolicy(mapping, lr_init: Union[List[float], float] = 0.001)\n</code></pre> <p>Base class for policy with mapping. Mapping can be a dict or a function (it should also be a list of latter types in case of multiple param groups). Mapping is the binding between epoch or step number and learning rate value.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <p>Binding of epoch or step number and learning rate.</p> required <code>lr_init</code> <code>Union[List[float], float]</code> <p>Initial learning rate for each group of parameters.</p> <code>0.001</code> Source code in <code>thunder/policy.py</code> <pre><code>def __init__(self, mapping, lr_init: Union[List[float], float] = 1e-3):\n\"\"\"\n    Base class for policy with mapping. Mapping can be a dict or a function\n    (it should also be a list of latter types in case of multiple param groups).\n    Mapping is the binding between epoch or step number and learning rate value.\n    Parameters\n    ----------\n    mapping\n        Binding of epoch or step number and learning rate.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\nself.current_mapping = None\nself.mapping = mapping\nself.current_lr_init = None\nself.lr_init = lr_init\nsuper().__init__()\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.current_mapping","title":"current_mapping  <code>instance-attribute</code>","text":"<pre><code>current_mapping = None\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.mapping","title":"mapping  <code>instance-attribute</code>","text":"<pre><code>mapping = mapping\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.current_lr_init","title":"current_lr_init  <code>instance-attribute</code>","text":"<pre><code>current_lr_init = None\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.lr_init","title":"lr_init  <code>instance-attribute</code>","text":"<pre><code>lr_init = lr_init\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.set_optimizer","title":"set_optimizer","text":"<pre><code>set_optimizer(optimizer: Optimizer) -&gt; None\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def set_optimizer(self, optimizer: Optimizer) -&gt; None:\nself.current_mapping = self.mapping\nif isinstance(self.mapping, dict) or callable(self.mapping):\nself.current_mapping = [deepcopy(self.mapping) for _ in optimizer.param_groups]\nself.current_lr_init = self.lr_init\nif isinstance(self.lr_init, (float, int)):\nself.current_lr_init = [self.lr_init for _ in optimizer.param_groups]\nif len(self.current_mapping) != len(optimizer.param_groups):\nraise ValueError(f\"Got {len(self.current_mapping)} mappings and {len(optimizer.param_groups)} param groups\")\nif len(self.current_lr_init) != len(optimizer.param_groups):\nraise ValueError(f\"Got {len(self.current_lr_init)} lr_init and {len(optimizer.param_groups)} param groups\")\nfor lr_init, param_group in zip(self.current_lr_init, optimizer.param_groups):\nparam_group[\"lr\"] = lr_init\nsuper().set_optimizer(optimizer)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>thunder/policy.py</code> <pre><code>def __repr__(self) -&gt; str:\nmapping = self.current_mapping if self.current_mapping else self.mapping\nlr_init = self.current_lr_init if self.current_lr_init is not None else self.lr_init\nreturn f\"{self.__class__.__name__}({mapping=}, {lr_init=})\"\n</code></pre>"}]}