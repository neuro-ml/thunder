{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>You saw the lightning. Now it's time to hear the thunder \ud83c\udf29\ufe0f</p>"},{"location":"#thunder","title":"Thunder","text":"<p>\ud83c\udf29\ufe0f The Deep Learning framework based on Lightning</p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install thunder\n</code></pre> <p>Currently thunder is not published on pypi. Install it via git clone.  </p>"},{"location":"#start-experimenting","title":"Start experimenting","text":"<p>Many frameworks provide you with interfaces for your models and training pipelines, but we  have yet to see any tools for creating whole experiment.</p> <p>With :thunder: it's as simple as 1, 2, 3:</p> <ol> <li> <p>Create a config (e.g. <code>base.config</code>):     <pre><code>from myproject import MyDataset, MyModule\nfrom lightning import Trainer\nfrom torch.utils.data import DataLoader\n# these 3 fields are required\ntrain_data = DataLoader(MyDataset())\nmodule = MyModule()\ntrainer = Trainer()\n</code></pre></p> </li> <li> <p>Build the experiment:    <pre><code>thunder build base.config /path/to/some/folder\n</code></pre></p> </li> <li>Run it     <pre><code>thunder run /path/to/some/folder\n</code></pre></li> </ol> <p>Also, 2 and 3 can be combined into a single command: <pre><code>thunder build-run base.config /path/to/some/folder\n</code></pre></p> <pre><code>$ thunder build base.config /path/to/some/folder\n</code></pre>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#thundermodule","title":"ThunderModule","text":""},{"location":"#metriclogger","title":"MetricLogger","text":""},{"location":"#experiment-configs","title":"Experiment configs","text":""},{"location":"#cli-integrations-with-wandb","title":"CLI &amp; Integrations with WandB","text":""},{"location":"utils/","title":"Utils","text":"<p>Some useful utility functions. </p>"},{"location":"utils/#thunder.torch.utils","title":"<code>thunder.torch.utils</code>","text":""},{"location":"utils/#thunder.torch.utils.get_device","title":"<code>get_device(x)</code>","text":"<p>Infer device of torch.Tensor or nn.Module instance. Parameters</p> <p>x: Union[torch.Tensor, nn.Module] Returns</p> <p>device: torch.device</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def get_device(x: Union[torch.Tensor, nn.Module]) -&gt; torch.device:\n\"\"\"\n    Infer device of torch.Tensor or nn.Module instance.\n    Parameters\n    ----------\n    x: Union[torch.Tensor, nn.Module]\n    Returns\n    -------\n    device: torch.device\n    \"\"\"\nif isinstance(x, (torch.Tensor, LightningModule)):\nreturn x.device\nelif isinstance(x, nn.Module):\ntry:\nreturn next(x.parameters()).device\nexcept StopIteration as e:\nraise RuntimeError(\"Can't infer the device, because the module has no parameters\") from e\nraise TypeError(f\"Can't infer the device of {type(x)}\")\n</code></pre>"},{"location":"utils/#thunder.torch.utils.maybe_from_np","title":"<code>maybe_from_np(*x, device='cpu')</code>","text":"<p>Recursively converts numpy arrays to torch.Tensor. Parameters</p> <p>*x: Any device: Union[torch.device, str]     Device to move to, default is CPU. Returns</p> <p>Collection of tensors. Examples</p> <p>x, y  # np.ndarray z = maybe_from_np(x) # convert to torch.Tensor x, y = maybe_from_np(x, y) # x and y are now tensors x, y, z = maybe_from_np(x, y, z) # maybe_from_np converts np arrays and tensors and does not affect other types dict_of_tensors = to_np(dict_of_np) # maybe_from_np converts any collection</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def maybe_from_np(*x: Any, device: Union[torch.device, str] = \"cpu\") -&gt; Any:\n\"\"\"\n    Recursively converts numpy arrays to torch.Tensor.\n    Parameters\n    ----------\n    *x: Any\n    device: Union[torch.device, str]\n        Device to move to, default is CPU.\n    Returns\n    -------\n    Collection of tensors.\n    Examples\n    -------\n    &gt;&gt;&gt; x, y  # np.ndarray\n    &gt;&gt;&gt; z = maybe_from_np(x) # convert to torch.Tensor\n    &gt;&gt;&gt; x, y = maybe_from_np(x, y) # x and y are now tensors\n    &gt;&gt;&gt; x, y, z = maybe_from_np(x, y, z) # maybe_from_np converts np arrays and tensors and does not affect other types\n    &gt;&gt;&gt; dict_of_tensors = to_np(dict_of_np) # maybe_from_np converts any collection\n    \"\"\"\ndef to_tensor(x):\nif isinstance(x, torch.Tensor):\nreturn x.to(device)\nreturn torch.from_numpy(x).to(device)\nreturn squeeze_first(apply_to_collection(x, (np.ndarray, np.generic, torch.Tensor), to_tensor))\n</code></pre>"},{"location":"utils/#thunder.torch.utils.tensor2np","title":"<code>tensor2np(x)</code>","text":"<p>Detaches, moves torch.Tensor to CPU and converts into numpy array. Parameters</p> <p>x: torch.Tensor Returns</p> <p>np.ndarray</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def tensor2np(x: torch.Tensor) -&gt; np.ndarray:\n\"\"\"\n    Detaches, moves torch.Tensor to CPU and converts into numpy array.\n    Parameters\n    ----------\n    x: torch.Tensor\n    Returns\n    -------\n    np.ndarray\n    \"\"\"\nreturn x.detach().cpu().numpy()\n</code></pre>"},{"location":"utils/#thunder.torch.utils.to_np","title":"<code>to_np(*x)</code>","text":"<p>Converts collection of tensors into numpy arrays. Parameters</p> <p>*x: Any Returns</p> <p>Collection of numpy arrays Examples</p> <p>x, y  # torch.Tensor z = to_np(x) # convert to numpy array x, y = to_np(x, y) # x and y are now numpy arrays x, y, z = to_np(x, y, z) # to_np converts only tensors and does not affect other types dict_of_np = to_np(dict_of_tensors) # to_np converts any collection</p> Source code in <code>thunder/torch/utils.py</code> <pre><code>def to_np(*x: Any) -&gt; Any:\n\"\"\"\n    Converts collection of tensors into numpy arrays.\n    Parameters\n    ----------\n    *x: Any\n    Returns\n    ----------\n    Collection of numpy arrays\n    Examples\n    --------\n    &gt;&gt;&gt; x, y  # torch.Tensor\n    &gt;&gt;&gt; z = to_np(x) # convert to numpy array\n    &gt;&gt;&gt; x, y = to_np(x, y) # x and y are now numpy arrays\n    &gt;&gt;&gt; x, y, z = to_np(x, y, z) # to_np converts only tensors and does not affect other types\n    &gt;&gt;&gt; dict_of_np = to_np(dict_of_tensors) # to_np converts any collection\n    \"\"\"\nreturn squeeze_first(apply_to_collection(x, torch.Tensor, tensor2np))\n</code></pre>"},{"location":"callbacks/","title":"Overview","text":"<p>Lightning Callbacks allow you to modify your training pipelines. For extra information see this.</p>"},{"location":"callbacks/#thunder-callbacks","title":"Thunder Callbacks","text":"Name Description MetricLogger Computes metrics and logs them TimeProfiler Logs the time of each LightningModule's step FailOnInterrupt Forces lightning Trainer to fail on KeyboardInterrupt"},{"location":"callbacks/fail_on_interrupt/","title":"FailOnInterrupt","text":"<p>Forces lightning Trainer to fail on KeyboardInterrupt by raising RuntimeError.</p>"},{"location":"callbacks/fail_on_interrupt/#usage","title":"Usage","text":"<pre><code>from lightning import Trainer\nfrom thunder.callbacks import FailOnInterrupt\ntrainer = Trainer(..., callbacks=[FailOnInterrupt()])\n</code></pre>"},{"location":"callbacks/fail_on_interrupt/#reference","title":"Reference","text":""},{"location":"callbacks/fail_on_interrupt/#thunder.callbacks.fail_on_interrupt.FailOnInterrupt","title":"<code>thunder.callbacks.fail_on_interrupt.FailOnInterrupt</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Forces RuntimeError in order for trainer to stop if KeyboardInterrupt was raised</p> Source code in <code>thunder/callbacks/fail_on_interrupt.py</code> <pre><code>class FailOnInterrupt(Callback):\n\"\"\"Forces RuntimeError in order for trainer to stop if KeyboardInterrupt was raised\"\"\"\ndef on_exception(self, trainer: Trainer, pl_module: LightningModule, exception: BaseException) -&gt; None:\nif isinstance(exception, KeyboardInterrupt):\nraise RuntimeError(\"Finished run on KeyboardInterrupt\") from exception\n</code></pre>"},{"location":"callbacks/fail_on_interrupt/#thunder.callbacks.fail_on_interrupt.FailOnInterrupt.on_exception","title":"<code>on_exception(trainer, pl_module, exception)</code>","text":"Source code in <code>thunder/callbacks/fail_on_interrupt.py</code> <pre><code>def on_exception(self, trainer: Trainer, pl_module: LightningModule, exception: BaseException) -&gt; None:\nif isinstance(exception, KeyboardInterrupt):\nraise RuntimeError(\"Finished run on KeyboardInterrupt\") from exception\n</code></pre>"},{"location":"callbacks/metric_logger/","title":"MetricLogger","text":"<p>This callback takes on computation and aggregation of the specified metrics.  </p>"},{"location":"callbacks/metric_logger/#usage","title":"Usage","text":""},{"location":"callbacks/metric_logger/#loss","title":"Loss","text":"<p>Despite the word <code>Metric</code> in the name, this callback also takes on logging of train loss(es). It casts them by the following rules: - If <code>loss</code> is of type <code>torch.Tensor</code> - <code>{\"loss\": loss}</code> is logged. - If <code>loss</code> is a list or tuple, then it is logged as <code>{\"i\": loss_i}</code>. - If <code>loss</code> is a dict, then it is logged as is.  </p> <p> </p> <p>All Tensors are cast to numpy arrays.</p> <p>At the end of epoch they are averaged and sent to logger.</p>"},{"location":"callbacks/metric_logger/#metric-computation","title":"Metric Computation","text":"<p>Metrics are assumed to be received as tuple <code>(X, Y)</code>, where X - batch of predictions, Y - batch of targets.  Further process of computation depends on whether <code>Group</code> or <code>Single</code> metrics are used. Also, there is no difference for MetricLogger between  <code>(X, Y)</code> and <code>((X,), (Y,))</code>. If your model has multiple outputs or requires multiple targets (e.g. neural network with 2 heads.), the output is expected to be <code>((X1, X2), (Y1, Y2))</code> (the most common way to represent such data in PyTorch), where X1 is batch of model's first output. In this case outputs will be recombined, so the first object of the output will be <code>((x1, x2), (y1, y2))</code>, where x1 is the first element of batch <code>X1</code>.  </p> <p> </p> <p>Inside the callback outputs are swapped, so if LightningModule returns (X, Y) then metrics will receive (Y, X).</p>"},{"location":"callbacks/metric_logger/#group-metrics","title":"Group metrics","text":"<p>Group metrics are computed on the entire dataset. For example, you want to compute classification accuracy on MNIST. <pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(group_metrics={\"accuracy\": accuracy_score})])\n</code></pre></p> <p>If you use any loggers (e.g. <code>Tensorboard</code> or <code>WandB</code>), <code>accuracy</code> will appear in them as follows: <code>val/accuracy</code> - validation metrics. <code>test/accuracy</code> - test metrics.</p> <p>You can also use preprocessing functions as keys of the dictionary. It is  covered in Preprocessing part in Single Metrics paragraph.</p>"},{"location":"callbacks/metric_logger/#single-metrics","title":"Single metrics","text":"<p>Single metrics are computed on each object separately and only then aggregated. It is a common use case for tasks like segmentation or object detection.</p>"},{"location":"callbacks/metric_logger/#simple-use-case","title":"Simple use case","text":"<p><pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(single_metrics={\"accuracy\": accuracy_score})])\n</code></pre> MetricLogger will log mean values by default. But you can add custom aggregations as well.</p>"},{"location":"callbacks/metric_logger/#custom-aggregations","title":"Custom aggregations","text":"<p>Let see what can be done if we want to log <code>std</code> of metrics as well as mean values. <pre><code>import numpy as np\nfrom thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\naggregate_fn = np.std\nmetric_logger = MetricLogger(single_metrics={\"accuracy\": accuracy_score},\naggregate_fn=aggregate_fn) \ntrainer = Trainer(callbacks=[metric_logger])\n</code></pre> The mean values appear in loggers with no additional keys.  MetricCallback will try to infer the name of an aggregating function and use it as an additional key.</p> <p><code>val/accuracy</code> - validation mean accuracy. <code>val/std/accuracy</code> - validation accuracy std. <code>test/accuracy</code> - test mean accuracy. <code>test/std/accuracy</code> - test accuracy std.</p> <p><code>aggregate_fn</code> can also be specified as follows:</p> <p><pre><code>import numpy as np\naggregate_fn = [np.std, np.median]\naggregate_fn = [np.std, \"median\", \"max\", \"min\"]\naggregate_fn = {\"zero\": lambda x: x[0]}\n</code></pre> MetricLogger can accept <code>str</code> or <code>List[str]</code> as <code>aggregate_fn</code>,  in this format it supports the following metrics:</p> Name Function \"median\" <code>np.median</code> \"min\" <code>np.min</code> \"max\" <code>np.max</code> \"std\" <code>np.std</code>"},{"location":"callbacks/metric_logger/#preprocessing","title":"Preprocessing","text":"<p>Sometimes metrics require some preprocessing. In this case, keys of <code>single_metrics</code> dict must be callable objects. <pre><code>from sklearn.metrics import accuracy_score, recall_score\nthreshold = lambda y, x: (y &gt; 0.5, x)\nsingle_metrics = {threshold: [accuracy_score, recall_score()]} \n# or\nsingle_metrics = {threshold: {\"acc\": accuracy_score, \"rec\": recall_score}}\n# or\nsingle_metrics = {threshold: recall_score}\n...\n</code></pre></p>"},{"location":"callbacks/metric_logger/#individual-metrics","title":"Individual Metrics","text":"<p>While computing <code>single_metrics</code>, one may appear in need of knowledge of metrics on each case. For this particular problem, the callback provides its users with <code>log_individual_metrics</code> flag. Being set to <code>True</code> it forces the callback to store table of metrics in the following format:</p> Name metric1 metric2 batch_idx0_0 some_value some_value batch_idx0_1 some_value some_value ... ... ... batch_idxn_m some_value some_value <p>For each set (e.g. <code>val</code>, <code>test</code>) and each <code>dataloader_idx</code>, MetricLogger stores separate table. By default aforementioned tables are saved to <code>default_root_dir</code> of lightning's Trainer, in the format of <code>set_name/dataloader_idx.csv</code> (e.g. <code>val/dataloader_0.csv</code>). If loggers you use have method <code>log_table</code> (e.g. <code>WandbLogger</code>),  then this method will receive key and each table in the format of <code>pd.DataFrame</code>. Code from <code>metric_logger.py</code>: <pre><code>logger.log_table(f\"{key}/dataloader_{dataloader_idx}\", dataframe=dataframe)\n</code></pre> where key is the current state of trainer (<code>val</code> or <code>test</code>).  </p> <p>Since lightning allows to use <code>batch_idx</code>, these indexes are used for metrics dataframes. But there can be more than one object in batch. To overcome this issue we iterate over batch and mark each object with the next index:  <pre><code>for i, object in enumerate(batch):\nobject_idx = f\"{batch_idx}_{i}\"\n</code></pre> If all batches consist of single object, then <code>\"_{i}\"</code> is removed.</p>"},{"location":"callbacks/metric_logger/#reference","title":"Reference","text":""},{"location":"callbacks/metric_logger/#thunder.callbacks.metric_logger.MetricLogger","title":"<code>thunder.callbacks.metric_logger.MetricLogger</code>","text":"<p>             Bases: <code>Callback</code></p>"},{"location":"callbacks/time_profiler/","title":"TimeProfiler","text":"<p>Lightning Callback which allows you to measure the time each step takes and log it during the training process.</p>"},{"location":"callbacks/time_profiler/#logged-values","title":"Logged values","text":"<p>TimeProfiler logs the following steps:</p> Name Logged by default Description train batch Time taken by forward, optimizer step, and backward during train step. validation batch Time taken by forward during validation step. train epoch Time taken by train epoch without validation. validation epoch Time taken by validation epoch. avg train downtime* Average downtime in training step. avg val downtime Average downtime in validation step. backward Time taken by backprop. optimizer step Time taken by optimizer. total train downtime Total downtime in training epoch. total val downtime Total downtime in validation epoch. <p>*Downtime - the process during which model does not work (e.g. data loader is working now)  </p>"},{"location":"callbacks/time_profiler/#usage","title":"Usage","text":"<pre><code>from thunder.callbacks import TimeProfiler\nfrom lightning import Trainer\n# logs default keys and in addition backward and optimizer step\ntrainer = Trainer(callbacks=[TimeProfiler(\"backward\", \"optimizer step\")])\n</code></pre>"},{"location":"callbacks/time_profiler/#reference","title":"Reference","text":""},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler","title":"<code>thunder.callbacks.time_profiler.TimeProfiler</code>","text":"<p>             Bases: <code>Callback</code></p>"},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler--parameters","title":"Parameters","text":"<p>keys : Union[str, bool]     Optional keys for logging. If set to <code>True</code> it will log all keys.</p>"},{"location":"cli/","title":"Command Line Interface","text":""},{"location":"cli/#requirements-lazycon","title":"Requirements: lazycon","text":"<p>Thunder provides its users with CLI to bring convenience and comfort into experiment building and execution routine.</p> <p>For any help you can use <pre><code>thunder --help\n</code></pre></p>"},{"location":"cli/#building-an-experiment","title":"Building an experiment","text":"<p>In order to build an experiment, you can execute the follwing command: <pre><code>thunder build /path/to/config /path/to/experiment\n</code></pre> It will create a folder with built configs in it. If <code>/path/to/experiment</code> already exists, thunder raises error.  In order to overwrite existing directory use <code>--overwrite</code> / <code>-o</code> flags.</p>"},{"location":"cli/#overriding-config-entries","title":"Overriding config entries","text":"<p>While conducting experiments one can find themselves in constant need of changing significant number of parameters. But it is not convenient to always do it via IDE or any other code editor.  Thunder gives an ability to override the values while building an experiment.</p> <p>If in your config you have <pre><code>batch_size = 1\nlr = 0.01\n</code></pre> You can override it using <code>-u</code> flag: <pre><code>thunder build /path/to/config /path/to/experiment -u batch_size=2 -u lr=0.001 </code></pre> <code>batch_size</code> and <code>lr</code> will be assigned 2 and 0.001 respectively.</p>"},{"location":"cli/#running-an-experiment","title":"Running an experiment","text":"<p>You can run built experiment by executing the next command: <pre><code>thunder run /path/to/experiment\n</code></pre> Under the hood thunder extracts necessary entries (e.g. model and trainer) from your built config and executes <code>trainer.run(model, train_data, ...)</code>.</p>"},{"location":"cli/#backend","title":"Backend","text":"<p>As default options Thunder supports several backends: - cli (default) - slurm</p> <p>You can switch between them by specifying <code>--backend</code> flag.  <pre><code>thunder run /path/to/experiment/ --backend slurm -c 4 -r 100G </code></pre> The command shown above will run SLURM job with 4 CPUs and 100G of RAM.</p>"},{"location":"cli/#predefined-run-configs","title":"Predefined run configs","text":"<p>You can predefine run configs to avoid reentering the same flags. Create <code>~/.config/thunder/backends.yml</code> (you can run <code>thunder show</code> in your terminal,  required path will be at the title of the table) in you home directory.  Now you can specify config name and its parameters: <pre><code>run_config_name:\nbackend: slurm\nconfig:\nram: 100G\ncpu: 4\ngpu: 1\npartition: partition_name\n</code></pre> In order to run an experiment with predefined parameters,  use <code>--backend</code> flag as in previous section:</p> <p><pre><code>thunder run /path/to/experiment/ --backend run_config_name\n</code></pre> You can overwrite parameters if you want to (e.g. 8 CPUs instead of 4): <pre><code>thunder run /path/to/experiment/ --backend run_config_name -c 8\n</code></pre></p>"},{"location":"cli/#add-set-list-remove","title":"Add, Set, List, Remove","text":"<p><code>thunder</code> CLI provides its users with built-in tools for managing their backends.</p> Command Description <code>thunder backend add</code> Add run config to the list of available configs. <code>thunder backend list</code> Show parameters of specified backend(s). <code>thunder backend remove</code> Delete backend from list. <code>thunder backend set</code> Set specified backend from list of available backends as default."},{"location":"cli/#examples","title":"Examples","text":""},{"location":"cli/#add","title":"add","text":"<p><pre><code>thunder backend add run_config_name backend=slurm ram=100 cpu=4 gpu=1 partition=partition_name\n</code></pre> If specified name already exists, you can use <code>--force</code> flag in order to overwrite it.  </p>"},{"location":"cli/#set","title":"set","text":"<pre><code>thunder backend set SOMENAME\nthunder backend list\n</code></pre>"},{"location":"cli/#list","title":"list","text":"<pre><code>thunder backend list NAME1 NAME2\n*shows backends with specified names*\n\nthunder backend list\n*shows all backends*\n</code></pre>"},{"location":"cli/#remove","title":"remove","text":"<pre><code>thunder backend remove SOMENAME\n</code></pre>"},{"location":"cli/#placeholders","title":"Placeholders","text":"<p>Some loggers and other tools in your experiment may require name  of the experiment. We find it convenient to use name of the folder you  build your experiment into as the name of the experiment for loggers.  Example with <code>WandbLogger</code>: <pre><code>from lightning.pytorch.loggers import WandbLogger\nfrom thunder.placeholders import ExpName, GroupName\nlogger = WandbLogger(name=ExpName, group=GroupName)\n</code></pre> In this case <code>GroupName</code> - name of the folder with built experiment and <code>ExpName</code> - name of the split.</p>"},{"location":"cli/#wandb-sweeps-integration","title":"WandB Sweeps integration","text":"<p>WandB has hyperparameters tuning system called Sweeps. Sweeps allow you to run multiple experiment with predefined grid of parameters and compare run results. However, we find default sweep execution system very inconvenient when it comes to running experiments on cluster.</p> <p>After running a few experiments with  WandB Logger,  you can create sweep configuration.  WandB will give a command <code>wandb agent project/sweep_id</code>. You can copy it and paste it into the following command: <pre><code>thunder PASTE_HERE /path/to/config /path/to/experiment </code></pre></p>"},{"location":"configs/","title":"Lazycon","text":"<p>Thunder embraces the power of lazycon allowing you to build configs for your  experiments. </p>"},{"location":"configs/#config-structure","title":"Config structure","text":"<p>Correct config should contain the following objects:</p> Name Required Description <code>trainer</code> Lightning Trainer instance. <code>module</code> LightningModule instance. <code>train_data</code> Loader of training data. <code>val_data</code> Loader of validation data. <code>test_data</code> Loader of test data. <code>datamodule</code> LightningDataModule instance, replaces <code>train_data</code>, <code>val_data</code> and <code>test_data</code> if specified."},{"location":"configs/#executing-a-config","title":"Executing a config","text":"<p>Thunder has its own Command Line Interface,  about which you can read here.</p>"},{"location":"core/thunder_module/","title":"ThunderModule","text":"<p>ThunderModule inherits everything from LightningModule and implements essential methods for most common training pipelines.</p>"},{"location":"core/thunder_module/#from-lightning-to-thunder","title":"From Lightning to Thunder","text":"<p>Most common pipelines are implemented in lightning in the following way: <pre><code>from lightning import LightningModule\nclass Model(LightningModule):\ndef __init__(self):\nself.architecture: nn.Module = ...\nself.metrics = ... # smth like Dict[str, Callable]\ndef forward(self, *args, **kwargs):\nreturn self.architecture(*args, **kwargs)\ndef criterion(self, x, y):\n...\ndef training_step(self, batch, batch_idx):\nx, y = batch\nreturn self.criterion(self(x), y)\ndef validation_step(self, batch, batch_idx, dataloader_idx):\n# forward and metrics computation or output preservation\n...\ndef test_step(self, batch, batch_idx, dataloader_idx):\n# forward and metrics computation or output preservation\n...\ndef configure_optimizers(self):\nreturn Adam(...), StepLR(...)\n</code></pre></p> <p>ThunderModule offers an implementation of necessary steps shown above. <pre><code>from thunder import ThunderModule\narchitecture: nn.Module = ...\ncriterion = CrossEntropy()\noptimizer = Adam(architecture.parameters())\nscheduler = StepLR(optimizer)\nmodel = ThunderModule(architecture, criterion,\noptimizer=optimizer, lr_scheduler=scheduler)\n</code></pre></p>"},{"location":"core/thunder_module/#configuring-optimizers","title":"Configuring Optimizers","text":"<p>For extra information see this. Lightning requires optimizers and learning rate policies to be defined inside <code>configure_optimizers</code> method. Using ThunderModule allows you to pass the following configurations of  optimizers and learning rate schedulers:</p> <pre><code>from torch import nn\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom torch.optim import Adam\narchitecture = nn.Linear(2, 2)\n</code></pre>"},{"location":"core/thunder_module/#no-scheduling","title":"No scheduling","text":"<pre><code>optimizer = Adam(architecture.parameters())\nmodel = ThunderModule(..., optimizer=optimizer)\n</code></pre>"},{"location":"core/thunder_module/#defining-optimizer-and-scheduler","title":"Defining optimizer and scheduler","text":"<pre><code>optimizer = Adam(architecture.parameters())\nlr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., optimizer=optimizer, lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#defining-no-optimizer","title":"Defining no optimizer","text":"<pre><code>lr_scheduler = LRScheduler(optimizer)\nmodel = ThunderModule(..., lr_scheduler=lr_scheduler)\n</code></pre>"},{"location":"core/thunder_module/#multiple-optimizers","title":"Multiple Optimizers","text":"<p>Thunder just as lightning supports configuration with more than 1 optimizer. If such configuration is to be used, manual optimization is required. Guide on manual optimization</p> <p>In thunder you can pass lists of optimizers and schedulers to ThunderModule. <pre><code>class ThunderModuleManual(ThunderModule):\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.automatic_optimization = False\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Scheduler(opt) for opt in optimizers]\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p>"},{"location":"core/thunder_module/#thunder-policies","title":"Thunder Policies","text":"<p>As shown above, torch schedulers require optimizer(s) to be passed to them before they are given to ThunderModule. It is not very convenient, and also they lack some basic  functionality. You can use thunder policies just like torch schedulers: <pre><code>from thunder.policy import Switch\noptimizers = [Adam(module1.parameters()), Adam(module2.parameters())]\nlr_schedulers = [Switch({1: 0.001}), Switch({2: 0.001})]\nmodel = ThunderModuleManual(..., optimizer=optimizers, lr_scheduler=lr_schedulers)\n</code></pre></p> <p>For extra information see Thunder Policies Docs.</p>"},{"location":"core/thunder_module/#inference","title":"Inference","text":"<p>During inference step, ThunderModule uses Predictors in order to preprocess data and make inverse transforms after passing data through the model. Default predictor is just an identity function.</p> <p>For more on predictors see Thunder Predictors Docs.</p>"},{"location":"core/thunder_module/#batch-transfer","title":"Batch Transfer","text":"<p>ThunderModule transfers training batches to device by default. However, during  inference batch remains on the device, on which it was received from data loader.  Transferring happens later in the <code>inference_step</code>, which is invoked in <code>validation_step</code>, <code>test_step</code> and <code>predict_step</code>.</p>"},{"location":"core/thunder_module/#reference","title":"Reference","text":""},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule","title":"<code>thunder.torch.core.ThunderModule</code>","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>thunder/torch/core.py</code> <pre><code>class ThunderModule(LightningModule):\ndef __init__(\nself,\narchitecture: nn.Module,\ncriterion: Callable,\nn_targets: int = 1,\nactivation: Callable = identity,\noptimizer: Union[List[Optimizer], Optimizer] = None,\nlr_scheduler: Union[List[LRScheduler], LRScheduler] = None,\npredictor: BasePredictor = None,\nn_val_targets: int = None\n):\n\"\"\"\n        Parameters\n        ----------\n        architecture: nn.Module\n            Model architecture used to conduct forward pass.\n        criterion: Callable\n            Criterion to optimize.\n        n_targets: int\n            Number of target values in train and inference batches, if negative, then ...\n        activation: Callable\n            Final activation function for inference, identity by default.\n        optimizer: Union[List[Optimizer], Optimizer]\n            Optimizers.\n        lr_scheduler: Union[List[LRScheduler], LRScheduler]\n            Learning Rate policies.\n        predictor: BasePredictor.\n            Predictor for inference.\n        n_val_targets: int\n            Number of target values for inference, if set to None assumes value of `n_targets`.\n        \"\"\"\nsuper().__init__()\nself.architecture = architecture\nself.criterion = criterion\nself.n_targets = n_targets\nself.n_val_targets = n_targets if n_val_targets is None else n_val_targets\nself.activation = activation\nself.optimizer = optimizer\nself.lr_scheduler = lr_scheduler\nself.predictor = predictor if predictor else Predictor()\ndef transfer_batch_to_device(self, batch: Tuple, device: torch.device, dataloader_idx: int) -&gt; Any:\nif self.trainer.state.stage != \"train\":\nreturn batch\nreturn super().transfer_batch_to_device(maybe_from_np(batch, device=device), device, dataloader_idx)\ndef forward(self, *args: Any, **kwargs: Any) -&gt; Any:\nreturn self.architecture(*args, **kwargs)\ndef training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT:\nx, y = batch[: -self.n_targets], batch[-self.n_targets:]\nreturn self.criterion(self(*x), *y)\ndef validation_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\ndef test_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\ndef predict_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\ndef predict(self, x) -&gt; STEP_OUTPUT:\n# TODO: do we need super(). ...?, also consider changing maybe_to_np to smth stricter\nx = maybe_from_np(x, device=self.device)\nif not isinstance(x, (list, tuple)):\nx = (x,)\nreturn to_np(self.activation(self(*x)))\ndef inference_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nx, y = map(squeeze_first, (batch[:-self.n_val_targets], batch[-self.n_val_targets:]))\nreturn self.predictor([x], self.predict)[0], y\ndef configure_optimizers(self) -&gt; Tuple[List[Optimizer], List[LRScheduler]]:\nif not self.optimizer and not self.lr_scheduler:\nraise NotImplementedError(\n\"You must specify optimizer or lr_scheduler, \"\n\"or implement configure_optimizers method\"\n)\n_optimizers = list(collapse([self.optimizer]))\n_lr_schedulers = list(collapse([self.lr_scheduler]))\nmax_len = max(map(len, (_optimizers, _lr_schedulers)))\n_optimizers = list(padded(_optimizers, None, max_len))\n_lr_schedulers = list(padded(_lr_schedulers, None, max_len))\noptimizers = []\nlr_schedulers = []\nfor optimizer, lr_scheduler in zip_equal(_optimizers, _lr_schedulers):\nif callable(lr_scheduler):\nif optimizer is None:\nraise ValueError(\"The scheduler demands an Optimizer, but received None\")\nlr_scheduler = lr_scheduler(optimizer)\noptimizers.append(optimizer if lr_scheduler is None else lr_scheduler.optimizer)\nif lr_scheduler is not None:\nlr_schedulers.append(lr_scheduler)\nif len(optimizers) &lt; len(lr_schedulers):\nraise ValueError(\n\"The number of optimizers must be greater or equal to the number of \"\nf\"lr_schedulers, got {len(optimizers)} and {len(lr_schedulers)}\\n\"\nf\"Optimizers: f{optimizers}\\n\"\nf\"Schedulers: f{lr_schedulers}\\n\"\n)\nreturn optimizers, lr_schedulers\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -&gt; STEP_OUTPUT:\nx, y = batch[: -self.n_targets], batch[-self.n_targets:]\nreturn self.criterion(self(*x), *y)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def validation_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def test_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; STEP_OUTPUT:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def predict_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nreturn self.inference_step(batch, batch_idx, dataloader_idx)\n</code></pre>"},{"location":"core/thunder_module/#thunder.torch.core.ThunderModule.inference_step","title":"<code>inference_step(batch, batch_idx, dataloader_idx=0)</code>","text":"Source code in <code>thunder/torch/core.py</code> <pre><code>def inference_step(self, batch: Tuple, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\nx, y = map(squeeze_first, (batch[:-self.n_val_targets], batch[-self.n_val_targets:]))\nreturn self.predictor([x], self.predict)[0], y\n</code></pre>"},{"location":"examples/","title":"Overview","text":"<p>Here are some examples of Thunder experiments.</p> Name Description Totalsegmentator LowRes Low resolution segmentation of liver in Totalsegmentator MNIST Classifier MNIST classifier config"},{"location":"examples/mnist/","title":"Mnist","text":"<pre><code>import numpy as np\nimport torch\nfrom lightning import Trainer\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import WandbLogger\nfrom sklearn.metrics import accuracy_score\nfrom thunder import ThunderModule\nfrom thunder.callbacks import MetricLogger\nfrom thunder.placeholders import ExpName, GroupName\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nBATCH_SIZE = 256\ntrain_ds = MNIST(\".\", train=True, download=True, transform=transforms.ToTensor())\nval_ds = MNIST(\".\", train=False, download=True, transform=transforms.ToTensor())\ntrain_data = DataLoader(train_ds, batch_size=BATCH_SIZE)\nval_data = DataLoader(val_ds, batch_size=BATCH_SIZE)\narchitecture = nn.Sequential(nn.Flatten(), torch.nn.Linear(28 * 28, 10))\nmodule = ThunderModule(\narchitecture, nn.CrossEntropyLoss(), optimizer=torch.optim.Adam(architecture.parameters())\n)\n# Initialize a trainer\ntrainer = Trainer(\ncallbacks=[ModelCheckpoint(save_last=True), MetricLogger(group_metrics={lambda y, x: (np.argmax(y), x): accuracy_score})],\naccelerator=\"auto\",\ndevices=1,\nmax_epochs=100,\nlogger=WandbLogger(\nname=ExpName,\ngroup=GroupName,\nproject=\"thunder-examples\",\nentity=\"arseniybelkov\",\n),\n)\n</code></pre>"},{"location":"examples/mnist/#source","title":"Source","text":"<p>Full source code is available at thunder-examples</p>"},{"location":"examples/totalsegm_lowres/","title":"Low Resolution Liver Segmentation","text":""},{"location":"examples/totalsegm_lowres/#requirements-deep-pipe-amid-connectome","title":"Requirements: deep-pipe, amid, connectome","text":"<p>Deep-Pipe was primarly used for metrics and batch combinations.</p>"},{"location":"examples/totalsegm_lowres/#main-config","title":"Main config","text":"<pre><code>from functools import partial\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom amid.totalsegmentator import Totalsegmentator\nfrom connectome import Apply, CacheToDisk, CacheToRam, Chain, Filter\nfrom dpipe import layers\nfrom dpipe.batch_iter import combine_pad\nfrom dpipe.im.metrics import dice_score, precision, recall\nfrom dpipe.torch.functional import weighted_cross_entropy_with_logits\nfrom lightning import Trainer\nfrom lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom lightning.pytorch.loggers import WandbLogger\nfrom sklearn.model_selection import train_test_split\nfrom thunder import ThunderModule\nfrom thunder.callbacks import MetricLogger, TimeProfiler\nfrom thunder.layout import SingleSplit\nfrom thunder.placeholders import ExpName, GroupName\nfrom thunder.policy import Switch\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom thunder_examples.dataset import (ConToTorch, NormalizeCT, RotateTotalsegm,\nZoom)\nSEED = 0xBadCafe\ntotalsegmentator = Totalsegmentator(\"/shared/data/Totalsegmentator_dataset.zip\")\\\n                    &gt;&gt; Filter(lambda study_type, split: study_type == \"ct abdomen-pelvis\" and split == \"train\")\npreprocessing = Chain(RotateTotalsegm(), Zoom(n=0.3), NormalizeCT(max_=200, min_=-200))\ndataset = Chain(totalsegmentator,\npreprocessing,\nCacheToRam())\nlayout = SingleSplit(dataset, train=0.7, val=0.3)\nbatch_size = 2\nbatches_per_epoch = 256\nmax_epochs = 200\ntrain_data = DataLoader(\nConToTorch(layout.train &gt;&gt; Apply(image=lambda x: x[None], liver=lambda x: x[None]), ['image', 'liver']),\nbatch_size=batch_size, num_workers=4,\nshuffle=True, collate_fn=partial(combine_pad, padding_values=np.min))\nval_data = DataLoader(\nConToTorch(layout.val &gt;&gt; Apply(image=lambda x: x[None], liver=lambda x: x[None]), ['image', 'liver']),\nbatch_size=batch_size, collate_fn=partial(combine_pad, padding_values=np.min), num_workers=4)\narchitecture = nn.Sequential(\nnn.Conv3d(1, 8, kernel_size=3, padding=1),\nlayers.FPN(\nlayers.ResBlock3d, nn.MaxPool3d(2), nn.Identity(),\nlayers.fpn.interpolate_merge(lambda x, y: torch.cat([x, y], 1), order=1),\n[\n[[8, 16, 16], [32, 16, 8]],\n[[16, 32, 32], [64, 32, 16]],\n[[32, 64, 64], [128, 64, 32]],\n[[64, 128, 128], [256, 128, 64]],\n[128, 256, 128],\n],\nkernel_size=3, padding=1,\n),\nlayers.PreActivation3d(8, 1, kernel_size=3, padding=1),\n)\ncriterion = weighted_cross_entropy_with_logits\nmodule = ThunderModule(architecture, criterion, activation=nn.Sigmoid(),\noptimizer=Adam(architecture.parameters()),\nlr_scheduler=Switch({0: 1e-3, 50: 1e-4, 150: 1e-5}))\ntrainer = Trainer(\ncallbacks=[\nMetricLogger({lambda y, x: (y &gt; 0.5, x &gt; 0.5): [precision, recall, dice_score]}, aggregate_fn=[\"std\", \"max\", \"min\"]),\nTimeProfiler(),\nLearningRateMonitor(\"epoch\"),\nModelCheckpoint(save_last=True),\n],\nlimit_train_batches=batches_per_epoch,\naccelerator='gpu', precision=16,\nmax_epochs=max_epochs,\nlogger=WandbLogger(name=ExpName, group=GroupName, project='thunder-examples', entity='arseniybelkov'))\n</code></pre>"},{"location":"examples/totalsegm_lowres/#contotorch","title":"ConToTorch","text":"<p>ConTotch is a wrapper for connectome dataset for it can be passed to torch DataLoader. <pre><code>from torch.utils.data import Dataset\nclass ConToTorch(Dataset):\ndef __init__(self, dataset, fields):\nself.loader = dataset._compile(fields)\nself.ids = dataset.ids\ndef __len__(self):\nreturn len(self.ids)\ndef __getitem__(self, item):\nreturn self.loader(self.ids[item])\n</code></pre></p>"},{"location":"examples/totalsegm_lowres/#source","title":"Source","text":"<p>Full source code is available at thunder-examples</p>"},{"location":"inference/","title":"Inference","text":"<p>For matters of inference, thunder offers Predictors - objects that  can transform data before and after model's inference.</p> <p>In ThunderModule Predictor is used as default inference runner.</p>"},{"location":"inference/#predictors","title":"Predictors","text":""},{"location":"inference/#thunder.predict.predict","title":"<code>thunder.predict.predict</code>","text":""},{"location":"inference/#thunder.predict.predict.BasePredictor","title":"<code>BasePredictor</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for all predictors.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class BasePredictor(ABC):\n\"\"\"Base class for all predictors.\"\"\"\n@abstractmethod\ndef forward(self, batches: Iterable) -&gt; Iterable:\n\"\"\"Process stream of batches before model inference.\"\"\"\nraise NotImplementedError(\"You must implement forward method\")\n@abstractmethod\ndef backward(self, predicts: Iterable) -&gt; Iterable:\n\"\"\"Post-process stream of predictions.\"\"\"\nraise NotImplementedError(\"You must implement backward method\")\ndef __call__(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\nreturn self.run(batches, predict_fn)\ndef run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n\"\"\"Runs preprocessing, inference and postprocessing.\"\"\"\nreturn self.backward(map(predict_fn, self.forward(batches)))\n</code></pre>"},{"location":"inference/#thunder.predict.predict.BasePredictor.backward","title":"<code>backward(predicts)</code>  <code>abstractmethod</code>","text":"<p>Post-process stream of predictions.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>@abstractmethod\ndef backward(self, predicts: Iterable) -&gt; Iterable:\n\"\"\"Post-process stream of predictions.\"\"\"\nraise NotImplementedError(\"You must implement backward method\")\n</code></pre>"},{"location":"inference/#thunder.predict.predict.BasePredictor.forward","title":"<code>forward(batches)</code>  <code>abstractmethod</code>","text":"<p>Process stream of batches before model inference.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>@abstractmethod\ndef forward(self, batches: Iterable) -&gt; Iterable:\n\"\"\"Process stream of batches before model inference.\"\"\"\nraise NotImplementedError(\"You must implement forward method\")\n</code></pre>"},{"location":"inference/#thunder.predict.predict.BasePredictor.run","title":"<code>run(batches, predict_fn)</code>","text":"<p>Runs preprocessing, inference and postprocessing.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>def run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\n\"\"\"Runs preprocessing, inference and postprocessing.\"\"\"\nreturn self.backward(map(predict_fn, self.forward(batches)))\n</code></pre>"},{"location":"inference/#thunder.predict.predict.Decorated","title":"<code>Decorated</code>","text":"<p>             Bases: <code>Predictor</code></p> <p>Decorates inference function Example</p> <p>Decorated(f, g, h)</p>"},{"location":"inference/#thunder.predict.predict.Decorated--inside-decorated","title":"inside Decorated","text":"<p>predict_fn = f(g(h(predict_fn)))</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class Decorated(Predictor):\n\"\"\"\n    Decorates inference function\n    Example\n    -----------\n    Decorated(f, g, h)\n    # inside Decorated\n    predict_fn = f(g(h(predict_fn)))\n    \"\"\"\ndef __init__(self, *decorators: Callable):\nself.decorators = compose(*decorators)\ndef run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\nreturn super().run(batches, self.decorators(predict_fn))\n</code></pre>"},{"location":"inference/#thunder.predict.predict.InfinitePredictor","title":"<code>InfinitePredictor</code>","text":"<p>             Bases: <code>BasePredictor</code></p> <p>Useful for running inference on infinite stream of data.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class InfinitePredictor(BasePredictor):\n\"\"\"Useful for running inference on infinite stream of data.\"\"\"\ndef forward(self, batches: Iterable) -&gt; Iterable:\nyield from batches\ndef backward(self, predicts: Iterable) -&gt; Iterable:\nyield from predicts\n</code></pre>"},{"location":"inference/#thunder.predict.predict.Predictor","title":"<code>Predictor</code>","text":"<p>             Bases: <code>InfinitePredictor</code></p> <p>Assumes using finite amount of data for inference to be run on.</p> Source code in <code>thunder/predict/predict.py</code> <pre><code>class Predictor(InfinitePredictor):\n\"\"\"Assumes using finite amount of data for inference to be run on.\"\"\"\ndef run(self, batches: Iterable, predict_fn: Callable) -&gt; Iterable:\nreturn tuple(super().run(batches, predict_fn))\n</code></pre>"},{"location":"layout/","title":"Layout","text":"<p>Layout instances are responsible for splitting your datasets and managing which data fold is used for each experiment. They also check reproducibility of your data splits.</p>"},{"location":"layout/#thunder-layouts","title":"Thunder Layouts","text":"Name Description Split Layout for K fold cross-validation SingleSplit Layout with several sets (e.g. train + val + test) <p>All Layout subclasses follow common interface </p>"},{"location":"layout/#thunder.layout.interface.Layout","title":"<code>thunder.layout.interface.Layout</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>thunder/layout/interface.py</code> <pre><code>class Layout(ABC):\n@abstractmethod\ndef build(self, experiment: Path, config: Config) -&gt; Iterable[Node]:\npass\n@abstractmethod\ndef load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\npass\n@abstractmethod\ndef set(self, **kwargs):\npass\n</code></pre>"},{"location":"layout/fixed/","title":"Fixed Splits","text":"<p>Fixed splits allow to create layouts from predefined splits of data.</p>"},{"location":"layout/fixed/#thunder.layout.fixed.FixedSplit","title":"<code>thunder.layout.fixed.FixedSplit</code>","text":"<p>             Bases: <code>Split</code></p> Source code in <code>thunder/layout/fixed.py</code> <pre><code>class FixedSplit(Split):\ndef __init__(self, splits: Sequence,\nnames: Optional[Sequence[str]] = None):\n\"\"\"\n        Creates experiment layout from given split.\n        Parameters\n        ----------\n        splits: Sequence\n            Split of data.\n        names: Optional[Sequence[str]]\n            Names of folds, e.g. 'train', 'val', test'.\n        \"\"\"\nif names is not None:\n# TODO\nassert len(set(names)) == len(names)\nassert len(splits[0]) == len(names)\nself.entries = sorted(set(collapse(splits)))\nself.splits = [tuple(fold) for fold in splits]\nself.names = names\nself.fold: Optional[int] = None\n</code></pre>"},{"location":"layout/fixed/#thunder.layout.fixed.FixedSingleSplit","title":"<code>thunder.layout.fixed.FixedSingleSplit</code>","text":"<p>             Bases: <code>SingleSplit</code></p> Source code in <code>thunder/layout/fixed.py</code> <pre><code>class FixedSingleSplit(SingleSplit):\ndef __init__(self, split: Union[Sequence, Dict[str, Sequence]],\nnames: Optional[Sequence[str]] = None):\n\"\"\"\n        Creates single fold experiment from given split.\n        Parameters\n        ----------\n        split: Union[Sequence, Dict[str, Sequence]]\n            split of data\n        Names of folds, e.g. 'train', 'val', test'. If data is of type `dict`,\n        then it is not required.\n        \"\"\"\nself.entries = sorted(set(collapse(split)))\nif isinstance(split, dict):\nnames = split.keys()  # from python3.7 order is guaranteed.\nsplit = split.values()\nself.split = dict(zip_equal(names, split))\n</code></pre>"},{"location":"layout/splits/","title":"Splits","text":""},{"location":"layout/splits/#thunder.layout.split.Split","title":"<code>thunder.layout.split.Split</code>","text":"<p>             Bases: <code>Layout</code></p> Source code in <code>thunder/layout/split.py</code> <pre><code>class Split(Layout):\ndef __init__(self, split: SplitType, entries: Sequence, *args: Any, names: Optional[Sequence[str]] = None,\n**kwargs: Any):\n\"\"\"\n        Splits data according to split function.\n        Parameters\n        ----------\n        split: Callable\n            Split function, or a sklearn splitter.\n        entries: Sequence\n            Series of ids or torch Dataset or Connectome Layer.\n        args: Any\n            args for split.\n        names: Optional[Sequence[str]]\n            Names of folds, e.g. 'train', 'val', test'\n        kwargs: Any\n        kwargs for split.\n        \"\"\"\nif not callable(split):\nif not hasattr(split, 'split'):\nraise TypeError(f'Expected either a function, or a sklearn splitter, got {type(split)!r}')\nsplit = split.split\nids = entries_to_ids(entries)\n# TODO: safer way to unify types\nsplits = [tuple(map(jsonify, xs)) for xs in split(ids, *args, **kwargs)]\nif names is not None:\n# TODO\nassert len(set(names)) == len(names)\nassert len(splits[0]) == len(names)\nself.entries = entries\nself.splits = splits\nself.names = names\nself.fold: Optional[int] = None\ndef __getitem__(self, item: int):\nreturn self._subset(item)\ndef __getattr__(self, name: str):\nif self.names is None:\nraise AttributeError(name)\nreturn self._subset(self.names.index(name))\ndef _subset(self, idx):\n# TODO\nassert self.fold is not None\nreturn entries_subset(self.entries, self.splits[self.fold][idx])\ndef build(self, experiment: Path, config: Config):\nconfig.dump(experiment / 'experiment.config')\nname = experiment.name\nfor fold, split in enumerate(self.splits):\nfolder = experiment / f'fold_{fold}'\nfolder.mkdir()\nsave(split, folder / 'split.json')\nlocal = config.copy().update(ExpName=f'{name}({fold})', GroupName=name)\nlocal.dump(folder / 'experiment.config')\nyield Node(name=str(fold))\ndef load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\nfolder = experiment / f'fold_{node.name}'\nreturn Config.load(folder / 'experiment.config'), folder, {\n'fold': int(node.name),\n'split': tuple(load(folder / 'split.json')),\n}\ndef set(self, fold: int, split: Optional[Sequence[Sequence]] = None):\nself.fold = fold\nif split is None:\nwarnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\nelse:\nif split != self.splits[fold]:\n# TODO: consistency error?\nraise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.build","title":"<code>build(experiment, config)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def build(self, experiment: Path, config: Config):\nconfig.dump(experiment / 'experiment.config')\nname = experiment.name\nfor fold, split in enumerate(self.splits):\nfolder = experiment / f'fold_{fold}'\nfolder.mkdir()\nsave(split, folder / 'split.json')\nlocal = config.copy().update(ExpName=f'{name}({fold})', GroupName=name)\nlocal.dump(folder / 'experiment.config')\nyield Node(name=str(fold))\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.load","title":"<code>load(experiment, node)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\nfolder = experiment / f'fold_{node.name}'\nreturn Config.load(folder / 'experiment.config'), folder, {\n'fold': int(node.name),\n'split': tuple(load(folder / 'split.json')),\n}\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.Split.set","title":"<code>set(fold, split=None)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def set(self, fold: int, split: Optional[Sequence[Sequence]] = None):\nself.fold = fold\nif split is None:\nwarnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\nelse:\nif split != self.splits[fold]:\n# TODO: consistency error?\nraise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit","title":"<code>thunder.layout.split.SingleSplit</code>","text":"<p>             Bases: <code>Layout</code></p> Source code in <code>thunder/layout/split.py</code> <pre><code>class SingleSplit(Layout):\ndef __init__(self, entries: Sequence, *, shuffle: bool = True,\nrandom_state: Union[np.random.RandomState, int, None] = 0,\n**sizes: Union[int, float]):\n\"\"\"\n        Creates single fold experiment, with custom number of sets.\n        Parameters\n        ----------\n        entries: Sequence\n            Sequence of ids or\n        shuffle: bool\n            Whether to shuffle entries.\n        random_state : Union[np.random.RandomState, int, None]\n        sizes: Union[int, float]\n        \"\"\"\nif not isinstance(random_state, np.random.RandomState):\nrandom_state = np.random.RandomState(random_state)\nids = entries_to_ids(entries)\nself.entries = entries\nself.split = dict(zip(sizes.keys(), multi_split(\nids, list(sizes.values()), shuffle=shuffle, random_state=random_state\n)))\ndef __getattr__(self, name: str):\nif name not in self.split:\nraise AttributeError(name)\nreturn entries_subset(self.entries, self.split[name])\ndef build(self, experiment: Path, config: Config):\nconfig.dump(experiment / 'experiment.config')\nname = experiment.name\nsave(self.split, experiment / 'split.json')\nlocal = config.copy().update(ExpName=name, GroupName=name)\nlocal.dump(experiment / 'experiment.config')\nreturn []\ndef load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\nreturn Config.load(experiment / 'experiment.config'), experiment, {\n'split': load(experiment / 'split.json'),\n}\ndef set(self, split: Optional[Dict[str, Sequence]] = None):\nif split is None:\nwarnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\nelse:\nif split != self.split:\n# TODO: consistency error?\nraise ValueError\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.build","title":"<code>build(experiment, config)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def build(self, experiment: Path, config: Config):\nconfig.dump(experiment / 'experiment.config')\nname = experiment.name\nsave(self.split, experiment / 'split.json')\nlocal = config.copy().update(ExpName=name, GroupName=name)\nlocal.dump(experiment / 'experiment.config')\nreturn []\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.load","title":"<code>load(experiment, node)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def load(self, experiment: Path, node: Optional[Node]) -&gt; Tuple[Config, Path, Dict[str, Any]]:\nreturn Config.load(experiment / 'experiment.config'), experiment, {\n'split': load(experiment / 'split.json'),\n}\n</code></pre>"},{"location":"layout/splits/#thunder.layout.split.SingleSplit.set","title":"<code>set(split=None)</code>","text":"Source code in <code>thunder/layout/split.py</code> <pre><code>def set(self, split: Optional[Dict[str, Sequence]] = None):\nif split is None:\nwarnings.warn('No reference split provided. Your results might be inconsistent!', UserWarning)\nelse:\nif split != self.split:\n# TODO: consistency error?\nraise ValueError\n</code></pre>"},{"location":"loggers/","title":"Loggers","text":"Name Description WandbLogger Slightly modified lightning WandbLogger"},{"location":"loggers/wandb/","title":"WandbLogger","text":"<p>Slightly modified WandbLogger. <code>thunder</code> version has additional parameter <code>remove_dead_duplicates</code>  that if being set to <code>True</code> (<code>False</code> by default), deletes all crashed or failed runs  with the same name and group within your project. <pre><code>from thunder.torch.loggers import WandbLogger\nlogger = WandbLogger(..., remove_dead_duplicates=True)\n</code></pre></p>"},{"location":"policy/","title":"Policy","text":"<p>Policies are objects that define how some value changes through time. Good example of them is Learning Rate Schedulers.  </p>"},{"location":"policy/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":"<p>Contrary to default PyTorch Learning Rate schedulers, ours does not require an optimizer to be passed during initialization.</p>"},{"location":"policy/#thunder-schedulers","title":"Thunder Schedulers","text":"Name Description Multiply Multiplies lr on each step by specified factor. Schedule Assigns lr values according to specified callable. Switch Assigns lr values according to specified dict schedule. <p>See Learning Rate Schedulers docs </p>"},{"location":"policy/lr_schedulers/","title":"LR Schedulers","text":"<p>All schedulers in thunder are subclasses of <code>torch.optim.lr_scheduler.LRScheduler</code>. However, during initialization they do not require optimizer to be passed.</p>"},{"location":"policy/lr_schedulers/#usage","title":"Usage","text":"<p>We will use Switch as an example. <pre><code>from thunder.policy import Switch\nswitch = Switch({10: 0.001, 20: 0.001 / 10})\n</code></pre> We have just created a policy, but to make it work, it still needs an optimizers. Let's see how it works after being assembled. <pre><code>optimizer = Adam(...)\nscheduler(optimizer) # bounds optimizer to scheduler\n# or \n# scheduler = scheduler(optimizer)\n# You can also retrieve optimizer:\nopt = scheduler.optimizer\n</code></pre> After assigning optimizer to scheduler, policy instance will work just like usual torch scheduler.</p>"},{"location":"policy/lr_schedulers/#initial-lr","title":"Initial LR","text":"<p>All schedulers have <code>lr_init</code> parameters, if specified, it will be used as lr value on 0th step.</p>"},{"location":"policy/lr_schedulers/#reference","title":"Reference","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Multiply","title":"<code>thunder.policy.Multiply</code>","text":"<p>             Bases: <code>MappingPolicy</code></p> <p>Multiplies learning rate value on the specified factor in <code>mapping</code>. Example:     <pre><code>    sch = Multiply({1: 0.1, 4: 0.3})\n</code></pre>     if initial learning rate is 1e-3, learning rate will be: 1e-3, 1e-4, 1e-4, 1e-4, 3-e5, ...</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Multiply--parameters","title":"Parameters","text":"<p>mapping: Union[List[Dict[int, float]], Dict[int, float]]     Maps epoch to factor, keeping the last value between the epochs. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Multiply(MappingPolicy):\n\"\"\"\n    Multiplies learning rate value on the specified factor in `mapping`.\n    Example:\n        ```python\n            sch = Multiply({1: 0.1, 4: 0.3})\n        ```\n        if initial learning rate is 1e-3, learning rate will be: 1e-3, 1e-4, 1e-4, 1e-4, 3-e5, ...\n    Parameters\n    ----------\n    mapping: Union[List[Dict[int, float]], Dict[int, float]]\n        Maps epoch to factor, keeping the last value between the epochs.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\nmapping: Union[List[Dict[int, float]], Dict[int, float]]\ndef get_lr(self) -&gt; List[float]:\nreturn [\nparam_group[\"lr\"] * mapping.get(self.last_epoch, 1)\nfor param_group, mapping in zip_equal(self.optimizer.param_groups, self.current_mapping)\n]\ndef state_dict(self) -&gt; Dict[str, Any]:\nreturn super().state_dict()\ndef load_state_dict(self, state_dict):\nsuper().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule","title":"<code>thunder.policy.Schedule</code>","text":"<p>             Bases: <code>MappingPolicy</code></p> <p>Assigns learning rate values received from callable mapping. Example:     <pre><code>sch = Schedule(np.cos)\n</code></pre>     lr will have values of np.cos(epoch_number)</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Schedule--parameters","title":"Parameters","text":"<p>mapping: Union[List[Callable], Callable]]     Maps epoch to value. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Schedule(MappingPolicy):\n\"\"\"\n    Assigns learning rate values received from callable mapping.\n    Example:\n        ```python\n        sch = Schedule(np.cos)\n        ```\n        lr will have values of np.cos(epoch_number)\n    Parameters\n    ----------\n    mapping: Union[List[Callable], Callable]]\n        Maps epoch to value.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\nmapping: Union[List[Callable], Callable]\ndef get_lr(self) -&gt; List[float]:\nreturn juxt(self.current_mapping)(self.last_epoch)\ndef state_dict(self) -&gt; Dict[str, Any]:\nreturn super().state_dict(\"mapping\")\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\nsuper().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch","title":"<code>thunder.policy.Switch</code>","text":"<p>             Bases: <code>MappingPolicy</code></p> <p>Assigns learning rate values received from dict mapping. Example:     <pre><code>sch = Switch({0: 1e-4, 2: 1e-10)\n</code></pre>     lr: 1e-4, 1e-4, 1e-10, 1e-10, ...</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Switch--parameters","title":"Parameters","text":"<p>mapping: Union[List[Dict[int, float]], Dict[int, float]]     Maps specified epochs to specified values, preserving learning rate between epochs. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Switch(MappingPolicy):\n\"\"\"\n    Assigns learning rate values received from dict mapping.\n    Example:\n        ```python\n        sch = Switch({0: 1e-4, 2: 1e-10)\n        ```\n        lr: 1e-4, 1e-4, 1e-10, 1e-10, ...\n    Parameters\n    ----------\n    mapping: Union[List[Dict[int, float]], Dict[int, float]]\n        Maps specified epochs to specified values, preserving learning rate between epochs.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\nmapping: Union[List[Dict[int, float]], Dict[int, float]]\ndef get_lr(self) -&gt; List[float]:\nreturn [\nmapping.get(self.last_epoch, param_group[\"lr\"])\nfor param_group, mapping in zip_equal(self.optimizer.param_groups, self.current_mapping)\n]\ndef state_dict(self) -&gt; Dict[str, Any]:\nreturn super().state_dict()\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\nsuper().load_state_dict(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#base-classes","title":"Base classes","text":""},{"location":"policy/lr_schedulers/#thunder.policy","title":"<code>thunder.policy</code>","text":""},{"location":"policy/lr_schedulers/#thunder.policy.Policy","title":"<code>Policy</code>","text":"<p>             Bases: <code>_LRScheduler</code></p> <p>Policy base class.</p> Source code in <code>thunder/policy.py</code> <pre><code>class Policy(LRScheduler, metaclass=ABCMeta):\n\"\"\"\n    Policy base class.\n    \"\"\"\ndef __init__(self):\npass\ndef __call__(self, optimizer: Optimizer) -&gt; Policy:\nself.set_optimizer(optimizer)\nreturn self\ndef set_optimizer(self, optimizer: Optimizer) -&gt; None:\n\"\"\"Assigns optimizer to a scheduler\"\"\"\nsuper().__init__(optimizer)\n@abstractmethod\ndef get_lr(self) -&gt; List[float]:\n\"\"\"\n        Computes new value of learning rate.\n        Returns\n        -------\n        List[float]\n        \"\"\"\npass\n@abstractmethod\ndef state_dict(self, *keys: str) -&gt; Dict[str, Any]:\n\"\"\"\n        Creates state dict of scheduler, excluding optimizer.\n        Parameters\n        ----------\n        keys: str\n            Names of attributes to be excluded from state_dict\n        Returns\n        -------\n        Dict[str, Any]\n        \"\"\"\nkeys = (*keys, \"optimizer\")\nreturn {key: value for key, value in self.__dict__.items() if key not in keys}\n@abstractmethod\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n\"\"\"\n        Loads state dict of scheduler\n        Parameters\n        ----------\n        state_dict: Dict[str, Any]\n            State dict of scheduler.\n        \"\"\"\nself.__dict__.update(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.get_lr","title":"<code>get_lr()</code>  <code>abstractmethod</code>","text":"<p>Computes new value of learning rate. Returns</p> <p>List[float]</p> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef get_lr(self) -&gt; List[float]:\n\"\"\"\n    Computes new value of learning rate.\n    Returns\n    -------\n    List[float]\n    \"\"\"\npass\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.load_state_dict","title":"<code>load_state_dict(state_dict)</code>  <code>abstractmethod</code>","text":"<p>Loads state dict of scheduler Parameters</p> <p>state_dict: Dict[str, Any]     State dict of scheduler.</p> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n\"\"\"\n    Loads state dict of scheduler\n    Parameters\n    ----------\n    state_dict: Dict[str, Any]\n        State dict of scheduler.\n    \"\"\"\nself.__dict__.update(state_dict)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.set_optimizer","title":"<code>set_optimizer(optimizer)</code>","text":"<p>Assigns optimizer to a scheduler</p> Source code in <code>thunder/policy.py</code> <pre><code>def set_optimizer(self, optimizer: Optimizer) -&gt; None:\n\"\"\"Assigns optimizer to a scheduler\"\"\"\nsuper().__init__(optimizer)\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.state_dict","title":"<code>state_dict(*keys)</code>  <code>abstractmethod</code>","text":"<p>Creates state dict of scheduler, excluding optimizer. Parameters</p> <p>keys: str     Names of attributes to be excluded from state_dict</p>"},{"location":"policy/lr_schedulers/#thunder.policy.Policy.state_dict--returns","title":"Returns","text":"<p>Dict[str, Any]</p> Source code in <code>thunder/policy.py</code> <pre><code>@abstractmethod\ndef state_dict(self, *keys: str) -&gt; Dict[str, Any]:\n\"\"\"\n    Creates state dict of scheduler, excluding optimizer.\n    Parameters\n    ----------\n    keys: str\n        Names of attributes to be excluded from state_dict\n    Returns\n    -------\n    Dict[str, Any]\n    \"\"\"\nkeys = (*keys, \"optimizer\")\nreturn {key: value for key, value in self.__dict__.items() if key not in keys}\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy","title":"<code>MappingPolicy</code>","text":"<p>             Bases: <code>Policy</code></p> Source code in <code>thunder/policy.py</code> <pre><code>class MappingPolicy(Policy, metaclass=ABCMeta):\ndef __init__(self, mapping, lr_init: Union[List[float], float] = 1e-3):\n\"\"\"\n        Base class for policy with mapping. Mapping can be a dict or a function\n        (it should also be a list of latter types in case of multiple param groups).\n        Mapping is the binding between epoch or step number and learning rate value.\n        Parameters\n        ----------\n        mapping\n            Binding of epoch or step number and learning rate.\n        lr_init: Union[List[float], float]]\n            Initial learning rate for each group of parameters.\n        \"\"\"\nself.current_mapping = None\nself.mapping = mapping\nself.current_lr_init = None\nself.lr_init = lr_init\nsuper().__init__()\ndef set_optimizer(self, optimizer: Optimizer) -&gt; None:\nself.current_mapping = self.mapping\nif isinstance(self.mapping, dict) or callable(self.mapping):\nself.current_mapping = [deepcopy(self.mapping) for _ in optimizer.param_groups]\nself.current_lr_init = self.lr_init\nif isinstance(self.lr_init, (float, int)):\nself.current_lr_init = [self.lr_init for _ in optimizer.param_groups]\nif len(self.current_mapping) != len(optimizer.param_groups):\nraise ValueError(f\"Got {len(self.current_mapping)} mappings and {len(optimizer.param_groups)} param groups\")\nif len(self.current_lr_init) != len(optimizer.param_groups):\nraise ValueError(f\"Got {len(self.current_lr_init)} lr_init and {len(optimizer.param_groups)} param groups\")\nfor lr_init, param_group in zip(self.current_lr_init, optimizer.param_groups):\nparam_group[\"lr\"] = lr_init\nsuper().set_optimizer(optimizer)\ndef __repr__(self) -&gt; str:\nmapping = self.current_mapping if self.current_mapping else self.mapping\nlr_init = self.current_lr_init if self.current_lr_init is not None else self.lr_init\nreturn f\"{self.__class__.__name__}({mapping=}, {lr_init=})\"\n</code></pre>"},{"location":"policy/lr_schedulers/#thunder.policy.MappingPolicy.__init__","title":"<code>__init__(mapping, lr_init=0.001)</code>","text":"<p>Base class for policy with mapping. Mapping can be a dict or a function (it should also be a list of latter types in case of multiple param groups). Mapping is the binding between epoch or step number and learning rate value. Parameters</p> <p>mapping     Binding of epoch or step number and learning rate. lr_init: Union[List[float], float]]     Initial learning rate for each group of parameters.</p> Source code in <code>thunder/policy.py</code> <pre><code>def __init__(self, mapping, lr_init: Union[List[float], float] = 1e-3):\n\"\"\"\n    Base class for policy with mapping. Mapping can be a dict or a function\n    (it should also be a list of latter types in case of multiple param groups).\n    Mapping is the binding between epoch or step number and learning rate value.\n    Parameters\n    ----------\n    mapping\n        Binding of epoch or step number and learning rate.\n    lr_init: Union[List[float], float]]\n        Initial learning rate for each group of parameters.\n    \"\"\"\nself.current_mapping = None\nself.mapping = mapping\nself.current_lr_init = None\nself.lr_init = lr_init\nsuper().__init__()\n</code></pre>"}]}