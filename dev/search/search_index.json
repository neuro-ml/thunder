{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>You saw the lightning. Now it's time to hear the thunder</p>"},{"location":"#thunder","title":"Thunder","text":"<p>The Deep Learning framework based on Lighnting</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install from pypi <pre><code>pip install thunder\n</code></pre> or directly from GitHub <pre><code>git clone https://github.com/neuro-ml/thunder.git\ncd thunder &amp;&amp; pip install -e .\n</code></pre></p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#thundermodule","title":"ThunderModule","text":""},{"location":"#metriclogger","title":"MetricLogger","text":""},{"location":"#experiment-configs","title":"Experiment configs","text":""},{"location":"#cli-integrations-with-wandb","title":"CLI &amp; Integrations with WandB","text":""},{"location":"callbacks/","title":"Callbacks","text":"<p>Lightning Callbacks allow you to modify your training pipelines. For extra information see this.</p>"},{"location":"callbacks/#thunder-callbacks","title":"Thunder Callbacks","text":"Name Description MetricLogger Computes metrics and logs them TimeProfiler Logs the time of each LightningModule's step"},{"location":"callbacks/metric_logger/","title":"Metric logger","text":""},{"location":"callbacks/metric_logger/#metriclogger","title":"MetricLogger","text":"<p>This callback takes on computation and aggregation of the specified metrics.  </p>"},{"location":"callbacks/metric_logger/#usage","title":"Usage","text":""},{"location":"callbacks/metric_logger/#group-metrics","title":"Group metrics","text":"<p>Group metrics are computed on the entire dataset. For example, you want to compute classification accuracy on MNIST. <pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(group_metrics={\"accuracy\": accuracy_score})])\n</code></pre></p> <p>If you use any loggers (e.g. <code>Tensorboard</code> or <code>WandB</code>), <code>accuracy</code> will appear in them as follows: <code>val/accuracy</code> - validation metrics. <code>test/accuracy</code> - test metrics.</p>"},{"location":"callbacks/metric_logger/#single-metrics","title":"Single metrics","text":"<p>Single metrics are computed on each object separately and only then aggregated. It is a common use case for tasks like segmentation or object detection.</p>"},{"location":"callbacks/metric_logger/#simple-use-case","title":"Simple use case","text":"<p><pre><code>from thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\ntrainer = Trainer(callbacks=[MetricLogger(single_metrics={\"accuracy\": accuracy_score})])\n</code></pre> MetricLogger will log mean values by default. But you can add custom aggregations as well.</p>"},{"location":"callbacks/metric_logger/#custom-aggregations","title":"Custom aggregations","text":"<p>Let see what can be done if we want to log <code>std</code> of metrics as well as mean values. <pre><code>import numpy as np\nfrom thunder.callbacks import MetricLogger\nfrom sklearn.metrics import accuracy_score\naggregate_fn = np.std\nmetric_logger = MetricLogger(single_metrics={\"accuracy\": accuracy_score},\naggregate_fn=aggregate_fn) \ntrainer = Trainer(callbacks=[metric_logger])\n</code></pre> The mean values appera in loggers with no additional keys.  MetricCallback will try to infer the name of an aggregating function and use it as an additional key.</p> <p><code>val/accuracy</code> - validation mean accuracy. <code>val/std/accuracy</code> - validation accuracy std. <code>test/accuracy</code> - test mean accuracy. <code>test/std/accuracy</code> - test accuracy std.</p> <p><code>aggregate_fn</code> can also be specified as follows:</p> <p><pre><code>import numpy as np\naggregate_fn = [np.std, np.median]\naggregate_fn = [np.std, \"median\", \"max\", \"min\"]\naggregate_fn = {\"zero\": lambda x: x[0]}\n</code></pre> MetricLogger can accept <code>str</code> or <code>List[str]</code> as <code>aggregate_fn</code>,  in this format it supports the following metrics:</p> Name Function \"median\" <code>np.median</code> \"min\" <code>np.min</code> \"max\" <code>np.max</code> \"std\" <code>np.std</code>"},{"location":"callbacks/metric_logger/#preprocessing","title":"Preprocessing","text":"<p>Sometimes metrics require some preprocessing. In this case, keys of <code>single_metrics</code> dict must be callable objects. <pre><code>from sklearn.metrics import accuracy_score, recall_score\nthreshold = lambda x, y: (x &gt; 0.5, y)\nsingle_metrics = {threshold: [accuracy_score, recall_score()]} \n# or\nsingle_metrics = {threshold: {\"acc\": accuracy_score, \"rec\": recall_score}}\n# or\nsingle_metrics = {threshold: recall_score}\n...\n</code></pre></p>"},{"location":"callbacks/metric_logger/#reference","title":"Reference","text":""},{"location":"callbacks/metric_logger/#thunder.callbacks.metric_logger.MetricLogger","title":"<code>thunder.callbacks.metric_logger.MetricLogger(single_metrics: Dict = None, group_metrics: Dict[str, Callable] = None, aggregate_fn: Union[Dict[str, Callable], str, Callable, List[Union[str, Callable]]] = None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>single_metrics</code> <code>Dict</code> <p>Metrics that are calculated on each object separately and then aggregated.</p> <code>None</code> <code>group_metrics</code> <code>Dict[str, Callable]</code> <p>Metrics that are calculated on entire dataset.</p> <code>None</code> <code>aggregate_fn</code> <code>Union[Dict[str, Callable], str, Callable, List[Union[str, Callable]]]</code> <p>How to aggregate metrics. By default computes mean value. If yoy specify somethind, then the callback will compute mean and the specified values.</p> <code>None</code>"},{"location":"callbacks/time_profiler/","title":"Time profiler","text":""},{"location":"callbacks/time_profiler/#timeprofiler","title":"TimeProfiler","text":"<p>Lightning Callback which allows you to measure the time each step takes and log it during the training process.</p>"},{"location":"callbacks/time_profiler/#logged-values","title":"Logged values","text":"<p>TimeProfiler logs the following steps:</p> Name Logged by default Description train batch Time taken by forward, optimizer step, and backward during train step. validation batch Time taken by forward during validation step. train epoch Time taken by train epoch without validation. validation epoch Time taken by validation epoch. avg train downtime* Average downtime in training step. avg val downtime Average downtime in validation step. backward Time taken by backprop. optimizer step Time taken by optimizer. total train downtime Total downtime in training epoch. total val downtime Total downtime in validation epoch. <p>*Downtime - the process during which model does not work (e.g. data loader is working now)  </p>"},{"location":"callbacks/time_profiler/#usage","title":"Usage","text":"<pre><code>from thunder.callbacks import TimeProfiler\nfrom lightning import Trainer\n# logs default keys and in addition backward and optimizer step\ntrainer = Trainer(callbacks=[TimeProfiler(\"backward\", \"optimizer step\")])\n</code></pre>"},{"location":"callbacks/time_profiler/#reference","title":"Reference","text":""},{"location":"callbacks/time_profiler/#thunder.callbacks.time_profiler.TimeProfiler","title":"<code>thunder.callbacks.time_profiler.TimeProfiler(*keys: Union[str, bool])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, bool]</code> <p>Optional keys for logging. If set to <code>True</code> it will log all keys.</p> <code>()</code>"},{"location":"core/thunder_module/","title":"ThunderModule","text":"<p>ThunderModule inherits everything from LightiningModule</p>"}]}